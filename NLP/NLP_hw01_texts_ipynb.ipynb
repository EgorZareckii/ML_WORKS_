{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNtLJlW4v5VF"
      },
      "source": [
        "## Классификация текстов с использованием предобученных языковых моделей.\n",
        "\n",
        "В данном задании вам предстоит обратиться к задаче классификации текстов и решить ее с использованием предобученной модели BERT."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "WsvH2280GWkV"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "# do not change the code in the block below\n",
        "# __________start of block__________\n",
        "import os\n",
        "import random\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from IPython.display import clear_output\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve\n",
        "\n",
        "%matplotlib inline\n",
        "# __________end of block__________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EynPWVCIGWkW"
      },
      "source": [
        "Обратимся к набору данных SST-2. Holdout часть данных (которая понадобится вам для посылки) доступна по ссылке ниже."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ams4A-f_GWkW",
        "outputId": "6d9107f7-3a94-480d-9398-3ffc4e378777",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-04-04 06:49:54--  https://raw.githubusercontent.com/girafe-ai/ml-course/refs/heads/24f_yandex_ml_trainings/homeworks/hw04_bert_and_co/texts_holdout.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 51581 (50K) [text/plain]\n",
            "Saving to: ‘texts_holdout.json’\n",
            "\n",
            "texts_holdout.json  100%[===================>]  50.37K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2025-04-04 06:49:54 (2.94 MB/s) - ‘texts_holdout.json’ saved [51581/51581]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# do not change the code in the block below\n",
        "# __________start of block__________\n",
        "\n",
        "!wget https://raw.githubusercontent.com/girafe-ai/ml-course/refs/heads/24f_yandex_ml_trainings/homeworks/hw04_bert_and_co/texts_holdout.json\n",
        "# __________end of block__________"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-iCmcUE7GWkW"
      },
      "outputs": [],
      "source": [
        "# do not change the code in the block below\n",
        "# __________start of block__________\n",
        "df = pd.read_csv(\n",
        "    \"https://github.com/clairett/pytorch-sentiment-classification/raw/master/data/SST2/train.tsv\",\n",
        "    delimiter=\"\\t\",\n",
        "    header=None,\n",
        ")\n",
        "texts_train = df[0].values[:5000]\n",
        "y_train = df[1].values[:5000]\n",
        "texts_test = df[0].values[5000:]\n",
        "y_test = df[1].values[5000:]\n",
        "with open(\"texts_holdout.json\") as iofile:\n",
        "    texts_holdout = json.load(iofile)\n",
        "# __________end of block__________"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "print(np.unique(y_train, return_counts=True))"
      ],
      "metadata": {
        "id": "CXMtsBcvGRi_",
        "outputId": "2b37a174-31db-4ae1-85f7-9f8b5f1732e2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(array([0, 1]), array([2393, 2607]))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.iloc[124][1]"
      ],
      "metadata": {
        "id": "-tvAqBUbKGyO",
        "outputId": "13566fc8-0af0-470c-953f-db33488b5b22",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.int64(1)"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "set(y_train)"
      ],
      "metadata": {
        "id": "hLFCoKPJvR7l",
        "outputId": "f07b7244-10de-4bd5-cc5c-869cce99e0da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{np.int64(0), np.int64(1)}"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qS_iGxlmGWkX"
      },
      "source": [
        "Весь остальной код предстоит написать вам.\n",
        "\n",
        "Для успешной сдачи на максимальный балл необходимо добиться хотя бы __84.5% accuracy на тестовой части выборки__."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Сначал разобьем текст на токены для берта. Это делается встроенным токенатором"
      ],
      "metadata": {
        "id": "fGN0HrnHO8JA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "collapsed": true,
        "id": "v8mMOuYFGWkX"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "from transformers import DataCollatorWithPadding\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=True,          # Включить маскирование\n",
        "    mlm_probability=0.15  # Вероятность маскирования (15% по умолчанию)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "  В берте когда мы обучаем его на понимании речи с помощью mask токенов labels - указывает токены типа MASK для подчета лосс. обычно такие токены это 103, в labels на их метсах стоят реальные токены а на остальных -100 чтобы не участовать в лоссе. Это нужно указать в datacollar. Модель для токена маски выдает вектор скрытого представления и из него строит вероятностное распределения слов в словаре и предсказывает наиболее вероятное слово.\n",
        "\n",
        "  Как будто бы в нашей задаче не нужно дообучать берт на таком подходе тк он уже обучен на массивах текста в разы больше нашего.\n",
        "\n",
        "  **Сначала просто попробуем обучить классификатор и соответственно дообучить сам берт на распознавании оуенки пользователя. **"
      ],
      "metadata": {
        "id": "M-k9EAvvcoST"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        inputs = self.tokenizer(\n",
        "            text,\n",
        "            max_length=self.max_len,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt' # возвращает тензор размерностью (1,batch_size)\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": inputs[\"input_ids\"].squeeze(0),  # Убираем batch-размерность [1, max_len] → [max_len]\n",
        "            \"attention_mask\": inputs[\"attention_mask\"].squeeze(0),\n",
        "        }"
      ],
      "metadata": {
        "id": "8evbh-4RLi2n"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "НЕ НАДО СМЕШИВАТЬ ЗАДАЧИ\n",
        "\n",
        "классификация и обучение берта на понимаии речи это разные задачи.\n",
        "\n",
        "когда решаем вторую то нужен еще колоратор чтобы сделать токены маскированными (в даталоудере указываем collate_fn=collator)\n",
        "\n",
        "Ну и для второй задачи надо использовать немнорго другубю модель специальнро для mlm так как там для каждого mask токена скрвтое его состаяние передается в классифкатор и предсказыввается слово. Довольнро сложно реализовать на базовой архитектуре\n"
      ],
      "metadata": {
        "id": "RMNELopPQSjV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "tokinize_text_train = CustomDataset(texts_train, y_train, tokenizer, max_len=40 )\n",
        "tokinize_text_test = CustomDataset(texts_test, y_test, tokenizer, max_len=40 )\n",
        "\n",
        "train_dl = DataLoader(tokinize_text_train, batch_size=128, '''collate_fn=data_collator''', shuffle=True) # пример для обучении на задаче mlm c collator\n",
        "test_dl = DataLoader(tokinize_text_test, batch_size=128, '''collate_fn=data_collator''', shuffle=False)"
      ],
      "metadata": {
        "id": "vW5rc16FNIcK"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokinize_text_train[0]['input_ids']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "muASIG-RkYbQ",
        "outputId": "bdbef59d-e379-4e4c-a62d-81db988908b3"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([  101,  1037, 18385,  1010,  6057,  1998,  2633, 18276,  2128, 16603,\n",
              "         1997,  5053,  1998,  1996,  6841,  1998,  5687,  5469,  3152,   102,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch = [tokinize_text_train[i] for i in range(2)]  # Берем 2 примера\n",
        "collated = data_collator(batch)\n",
        "print(collated['input_ids'].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1LihRxGh9J6",
        "outputId": "bb111650-3b56-4f9e-9a42-f90642a31b48"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 40])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokinize_text_train[125]"
      ],
      "metadata": {
        "id": "j-a8DdT2NbcO",
        "outputId": "2cfeb10c-4504-4062-8127-4d888f9a642e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([  101,  2720, 15616,  2003,  1010,  2004,  4038,  3632,  1010,  2200,\n",
              "         10021,  1998,  1999,  1996,  2190,  2126,   102,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]),\n",
              " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for batch, label in train_dl:\n",
        "  print(batch['input_ids'].size())\n",
        "  break"
      ],
      "metadata": {
        "id": "PmkXEryPo1JD",
        "outputId": "876035a9-f6f7-4ee1-941e-125e63b6d4dd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([128, 40])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Вообще есть уже встроенный класс для классифкации в берт. Однако работает он буквально как мой код, поэтому ради практики напишу свой"
      ],
      "metadata": {
        "id": "q-KLRDJQjexQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BertClassifier1(nn.Module):\n",
        "    def __init__(self, bert):\n",
        "        super().__init__()\n",
        "        self.bert = bert\n",
        "\n",
        "        # Усиленная классификационная головка\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(0.3),  # Увеличиваем dropout\n",
        "            nn.Linear(768, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(256),  # Добавляем BatchNorm\n",
        "            nn.Dropout(0.2),     # Дополнительный dropout\n",
        "            nn.Linear(256, 1)\n",
        "        )\n",
        "\n",
        "        # Правильная инициализация весов\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for layer in self.classifier:\n",
        "            if isinstance(layer, nn.Linear):\n",
        "                nn.init.kaiming_normal_(layer.weight)\n",
        "                nn.init.zeros_(layer.bias)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            output_hidden_states=True  # Получаем все скрытые состояния\n",
        "        )\n",
        "\n",
        "        # Вариант 1: Используем pooler_output\n",
        "        pooled_output = outputs.pooler_output\n",
        "\n",
        "        return self.classifier(pooled_output)"
      ],
      "metadata": {
        "id": "9nw5Faeyhle5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokinize_text_train[0][0]"
      ],
      "metadata": {
        "id": "LpUYWguWvotn",
        "outputId": "c2b172b8-21dc-4853-a141-a2f6dcb45c66",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([  101,  1037, 18385,  1010,  6057,  1998,  2633, 18276,  2128, 16603,\n",
              "          1997,  5053,  1998,  1996,  6841,  1998,  5687,  5469,  3152,   102,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]),\n",
              " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Размер словаря токенизатора: {tokenizer.vocab_size}\")\n",
        "print(f\"Размер embedding-слоя модели: {model.bert.embeddings.word_embeddings.num_embeddings}\")"
      ],
      "metadata": {
        "id": "uZMF-XtFzhv0",
        "outputId": "507bed78-3c9f-4400-c1e6-0748e34b824e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Размер словаря токенизатора: 30522\n",
            "Размер embedding-слоя модели: 30522\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = BertClassifier1(bert)\n",
        "device = 'cuda'\n",
        "model.to(device)\n",
        "for i, d in train_dl:\n",
        "\n",
        "  input_ids = i['input_ids'].to(device)\n",
        "  attention_mask = i['attention_mask'].to(device)\n",
        "  output = model(input_ids, attention_mask)\n",
        "  print(output.shape)\n",
        "  print(input_ids.shape)\n",
        "  print(d.shape)\n",
        "  break"
      ],
      "metadata": {
        "collapsed": true,
        "id": "cDCSxjo2i1xR",
        "outputId": "dbcf6b5d-ba46-431c-f71b-f2ca064a034b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([128, 1])\n",
            "torch.Size([128, 40])\n",
            "torch.Size([128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Обучение"
      ],
      "metadata": {
        "id": "uV0r7QtfjqWP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import torch\n",
        "def train(model, optimizer, criterion, train_dl, test_dl):\n",
        "  model.train()\n",
        "  for epoch in range(10):\n",
        "    train_losses = 0\n",
        "    norm = 0\n",
        "    accuracy = 0\n",
        "    model.to(device)\n",
        "    total_acc = 0\n",
        "    for data, label in tqdm(train_dl, desc=\"Training\"):\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      input_ids = data['input_ids'].to(device)\n",
        "      attention_mask = data['attention_mask'].to(device)\n",
        "\n",
        "      label = label.float().unsqueeze(1).to(device)\n",
        "      output = model(input_ids, attention_mask)\n",
        "      loss = criterion(output, label)\n",
        "      loss.backward()\n",
        "      train_losses += loss.item()*input_ids.size(0)\n",
        "      norm += input_ids.size(0)\n",
        "\n",
        "      preds = (output >= 0.5).float()\n",
        "      acc = (preds == label).float().mean()\n",
        "      total_acc += acc.item()\n",
        "\n",
        "      optimizer.step()\n",
        "\n",
        "    print(f\"Train loss: {train_losses/norm}\", total_acc/norm)\n",
        "\n",
        "    model.eval()\n",
        "    test_losses = 0\n",
        "    norm = 0\n",
        "    with torch.no_grad():\n",
        "      for data, label in tqdm(test_dl, desc=\"Testing\"):\n",
        "        input_ids = data['input_ids'].to(device)\n",
        "        attention_mask = data['attention_mask'].to(device)\n",
        "\n",
        "        label = label.float().unsqueeze(1).to(device)\n",
        "\n",
        "        output = model(input_ids, attention_mask)\n",
        "        loss = criterion(output, label)\n",
        "        test_losses += loss.item()*input_ids.size(0)\n",
        "        norm += input_ids.size(0)\n",
        "\n",
        "    print(f\"Test loss: {test_losses/norm}\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "s880re7WjrP8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Полное обучение модели сол всеми размороженными весами дает плохой результат и забывание. Заморозим Берт"
      ],
      "metadata": {
        "id": "YakOBMLB1M2J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertModel\n",
        "\n",
        "bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "model = BertClassifier1(bert)\n",
        "\n",
        "\n",
        "for params in model.bert.parameters():\n",
        "  params.requires_grad = False\n",
        "\n",
        "for name, param in model.named_parameters():\n",
        "    print(f\"Параметр {name} обучаемый: {param.requires_grad}\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ZFJ4oZmU1Wwj",
        "outputId": "0764324a-80e8-4937-f616-6b0e3b17ffca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Параметр bert.embeddings.word_embeddings.weight обучаемый: False\n",
            "Параметр bert.embeddings.position_embeddings.weight обучаемый: False\n",
            "Параметр bert.embeddings.token_type_embeddings.weight обучаемый: False\n",
            "Параметр bert.embeddings.LayerNorm.weight обучаемый: False\n",
            "Параметр bert.embeddings.LayerNorm.bias обучаемый: False\n",
            "Параметр bert.encoder.layer.0.attention.self.query.weight обучаемый: False\n",
            "Параметр bert.encoder.layer.0.attention.self.query.bias обучаемый: False\n",
            "Параметр bert.encoder.layer.0.attention.self.key.weight обучаемый: False\n",
            "Параметр bert.encoder.layer.0.attention.self.key.bias обучаемый: False\n",
            "Параметр bert.encoder.layer.0.attention.self.value.weight обучаемый: False\n",
            "Параметр bert.encoder.layer.0.attention.self.value.bias обучаемый: False\n",
            "Параметр bert.encoder.layer.0.attention.output.dense.weight обучаемый: False\n",
            "Параметр bert.encoder.layer.0.attention.output.dense.bias обучаемый: False\n",
            "Параметр bert.encoder.layer.0.attention.output.LayerNorm.weight обучаемый: False\n",
            "Параметр bert.encoder.layer.0.attention.output.LayerNorm.bias обучаемый: False\n",
            "Параметр bert.encoder.layer.0.intermediate.dense.weight обучаемый: False\n",
            "Параметр bert.encoder.layer.0.intermediate.dense.bias обучаемый: False\n",
            "Параметр bert.encoder.layer.0.output.dense.weight обучаемый: False\n",
            "Параметр bert.encoder.layer.0.output.dense.bias обучаемый: False\n",
            "Параметр bert.encoder.layer.0.output.LayerNorm.weight обучаемый: False\n",
            "Параметр bert.encoder.layer.0.output.LayerNorm.bias обучаемый: False\n",
            "Параметр bert.encoder.layer.1.attention.self.query.weight обучаемый: False\n",
            "Параметр bert.encoder.layer.1.attention.self.query.bias обучаемый: False\n",
            "Параметр bert.encoder.layer.1.attention.self.key.weight обучаемый: False\n",
            "Параметр bert.encoder.layer.1.attention.self.key.bias обучаемый: False\n",
            "Параметр bert.encoder.layer.1.attention.self.value.weight обучаемый: False\n",
            "Параметр bert.encoder.layer.1.attention.self.value.bias обучаемый: False\n",
            "Параметр bert.encoder.layer.1.attention.output.dense.weight обучаемый: False\n",
            "Параметр bert.encoder.layer.1.attention.output.dense.bias обучаемый: False\n",
            "Параметр bert.encoder.layer.1.attention.output.LayerNorm.weight обучаемый: False\n",
            "Параметр bert.encoder.layer.1.attention.output.LayerNorm.bias обучаемый: False\n",
            "Параметр bert.encoder.layer.1.intermediate.dense.weight обучаемый: False\n",
            "Параметр bert.encoder.layer.1.intermediate.dense.bias обучаемый: False\n",
            "Параметр bert.encoder.layer.1.output.dense.weight обучаемый: False\n",
            "Параметр bert.encoder.layer.1.output.dense.bias обучаемый: False\n",
            "Параметр bert.encoder.layer.1.output.LayerNorm.weight обучаемый: False\n",
            "Параметр bert.encoder.layer.1.output.LayerNorm.bias обучаемый: False\n",
            "Параметр bert.encoder.layer.2.attention.self.query.weight обучаемый: False\n",
            "Параметр bert.encoder.layer.2.attention.self.query.bias обучаемый: False\n",
            "Параметр bert.encoder.layer.2.attention.self.key.weight обучаемый: False\n",
            "Параметр bert.encoder.layer.2.attention.self.key.bias обучаемый: False\n",
            "Параметр bert.encoder.layer.2.attention.self.value.weight обучаемый: False\n",
            "Параметр bert.encoder.layer.2.attention.self.value.bias обучаемый: False\n",
            "Параметр bert.encoder.layer.2.attention.output.dense.weight обучаемый: False\n",
            "Параметр bert.encoder.layer.2.attention.output.dense.bias обучаемый: False\n",
            "Параметр bert.encoder.layer.2.attention.output.LayerNorm.weight обучаемый: False\n",
            "Параметр bert.encoder.layer.2.attention.output.LayerNorm.bias обучаемый: False\n",
            "Параметр bert.encoder.layer.2.intermediate.dense.weight обучаемый: False\n",
            "Параметр bert.encoder.layer.2.intermediate.dense.bias обучаемый: False\n",
            "Параметр bert.encoder.layer.2.output.dense.weight обучаемый: False\n",
            "Параметр bert.encoder.layer.2.output.dense.bias обучаемый: False\n",
            "Параметр bert.encoder.layer.2.output.LayerNorm.weight обучаемый: False\n",
            "Параметр bert.encoder.layer.2.output.LayerNorm.bias обучаемый: False\n",
            "Параметр bert.encoder.layer.3.attention.self.query.weight обучаемый: False\n",
            "Параметр bert.encoder.layer.3.attention.self.query.bias обучаемый: False\n",
            "Параметр bert.encoder.layer.3.attention.self.key.weight обучаемый: False\n",
            "Параметр bert.encoder.layer.3.attention.self.key.bias обучаемый: False\n",
            "Параметр bert.encoder.layer.3.attention.self.value.weight обучаемый: False\n",
            "Параметр bert.encoder.layer.3.attention.self.value.bias обучаемый: False\n",
            "Параметр bert.encoder.layer.3.attention.output.dense.weight обучаемый: False\n",
            "Параметр bert.encoder.layer.3.attention.output.dense.bias обучаемый: False\n",
            "Параметр bert.encoder.layer.3.attention.output.LayerNorm.weight обучаемый: False\n",
            "Параметр bert.encoder.layer.3.attention.output.LayerNorm.bias обучаемый: False\n",
            "Параметр bert.encoder.layer.3.intermediate.dense.weight обучаемый: False\n",
            "Параметр bert.encoder.layer.3.intermediate.dense.bias обучаемый: False\n",
            "Параметр bert.encoder.layer.3.output.dense.weight обучаемый: False\n",
            "Параметр bert.encoder.layer.3.output.dense.bias обучаемый: False\n",
            "Параметр bert.encoder.layer.3.output.LayerNorm.weight обучаемый: False\n",
            "Параметр bert.encoder.layer.3.output.LayerNorm.bias обучаемый: False\n",
            "Параметр bert.encoder.layer.4.attention.self.query.weight обучаемый: False\n",
            "Параметр bert.encoder.layer.4.attention.self.query.bias обучаемый: False\n",
            "Параметр bert.encoder.layer.4.attention.self.key.weight обучаемый: False\n",
            "Параметр bert.encoder.layer.4.attention.self.key.bias обучаемый: False\n",
            "Параметр bert.encoder.layer.4.attention.self.value.weight обучаемый: False\n",
            "Параметр bert.encoder.layer.4.attention.self.value.bias обучаемый: False\n",
            "Параметр bert.encoder.layer.4.attention.output.dense.weight обучаемый: False\n",
            "Параметр bert.encoder.layer.4.attention.output.dense.bias обучаемый: False\n",
            "Параметр bert.encoder.layer.4.attention.output.LayerNorm.weight обучаемый: False\n",
            "Параметр bert.encoder.layer.4.attention.output.LayerNorm.bias обучаемый: False\n",
            "Параметр bert.encoder.layer.4.intermediate.dense.weight обучаемый: False\n",
            "Параметр bert.encoder.layer.4.intermediate.dense.bias обучаемый: False\n",
            "Параметр bert.encoder.layer.4.output.dense.weight обучаемый: False\n",
            "Параметр bert.encoder.layer.4.output.dense.bias обучаемый: False\n",
            "Параметр bert.encoder.layer.4.output.LayerNorm.weight обучаемый: False\n",
            "Параметр bert.encoder.layer.4.output.LayerNorm.bias обучаемый: False\n",
            "Параметр bert.encoder.layer.5.attention.self.query.weight обучаемый: False\n",
            "Параметр bert.encoder.layer.5.attention.self.query.bias обучаемый: False\n",
            "Параметр bert.encoder.layer.5.attention.self.key.weight обучаемый: False\n",
            "Параметр bert.encoder.layer.5.attention.self.key.bias обучаемый: False\n",
            "Параметр bert.encoder.layer.5.attention.self.value.weight обучаемый: False\n",
            "Параметр bert.encoder.layer.5.attention.self.value.bias обучаемый: False\n",
            "Параметр bert.encoder.layer.5.attention.output.dense.weight обучаемый: False\n",
            "Параметр bert.encoder.layer.5.attention.output.dense.bias обучаемый: False\n",
            "Параметр bert.encoder.layer.5.attention.output.LayerNorm.weight обучаемый: False\n",
            "Параметр bert.encoder.layer.5.attention.output.LayerNorm.bias обучаемый: False\n",
            "Параметр bert.encoder.layer.5.intermediate.dense.weight обучаемый: False\n",
            "Параметр bert.encoder.layer.5.intermediate.dense.bias обучаемый: False\n",
            "Параметр bert.encoder.layer.5.output.dense.weight обучаемый: False\n",
            "Параметр bert.encoder.layer.5.output.dense.bias обучаемый: False\n",
            "Параметр bert.encoder.layer.5.output.LayerNorm.weight обучаемый: False\n",
            "Параметр bert.encoder.layer.5.output.LayerNorm.bias обучаемый: False\n",
            "Параметр bert.encoder.layer.6.attention.self.query.weight обучаемый: False\n",
            "Параметр bert.encoder.layer.6.attention.self.query.bias обучаемый: False\n",
            "Параметр bert.encoder.layer.6.attention.self.key.weight обучаемый: False\n",
            "Параметр bert.encoder.layer.6.attention.self.key.bias обучаемый: False\n",
            "Параметр bert.encoder.layer.6.attention.self.value.weight обучаемый: False\n",
            "Параметр bert.encoder.layer.6.attention.self.value.bias обучаемый: False\n",
            "Параметр bert.encoder.layer.6.attention.output.dense.weight обучаемый: False\n",
            "Параметр bert.encoder.layer.6.attention.output.dense.bias обучаемый: False\n",
            "Параметр bert.encoder.layer.6.attention.output.LayerNorm.weight обучаемый: False\n",
            "Параметр bert.encoder.layer.6.attention.output.LayerNorm.bias обучаемый: False\n",
            "Параметр bert.encoder.layer.6.intermediate.dense.weight обучаемый: False\n",
            "Параметр bert.encoder.layer.6.intermediate.dense.bias обучаемый: False\n",
            "Параметр bert.encoder.layer.6.output.dense.weight обучаемый: False\n",
            "Параметр bert.encoder.layer.6.output.dense.bias обучаемый: False\n",
            "Параметр bert.encoder.layer.6.output.LayerNorm.weight обучаемый: False\n",
            "Параметр bert.encoder.layer.6.output.LayerNorm.bias обучаемый: False\n",
            "Параметр bert.encoder.layer.7.attention.self.query.weight обучаемый: False\n",
            "Параметр bert.encoder.layer.7.attention.self.query.bias обучаемый: False\n",
            "Параметр bert.encoder.layer.7.attention.self.key.weight обучаемый: False\n",
            "Параметр bert.encoder.layer.7.attention.self.key.bias обучаемый: False\n",
            "Параметр bert.encoder.layer.7.attention.self.value.weight обучаемый: False\n",
            "Параметр bert.encoder.layer.7.attention.self.value.bias обучаемый: False\n",
            "Параметр bert.encoder.layer.7.attention.output.dense.weight обучаемый: False\n",
            "Параметр bert.encoder.layer.7.attention.output.dense.bias обучаемый: False\n",
            "Параметр bert.encoder.layer.7.attention.output.LayerNorm.weight обучаемый: False\n",
            "Параметр bert.encoder.layer.7.attention.output.LayerNorm.bias обучаемый: False\n",
            "Параметр bert.encoder.layer.7.intermediate.dense.weight обучаемый: False\n",
            "Параметр bert.encoder.layer.7.intermediate.dense.bias обучаемый: False\n",
            "Параметр bert.encoder.layer.7.output.dense.weight обучаемый: False\n",
            "Параметр bert.encoder.layer.7.output.dense.bias обучаемый: False\n",
            "Параметр bert.encoder.layer.7.output.LayerNorm.weight обучаемый: False\n",
            "Параметр bert.encoder.layer.7.output.LayerNorm.bias обучаемый: False\n",
            "Параметр bert.encoder.layer.8.attention.self.query.weight обучаемый: False\n",
            "Параметр bert.encoder.layer.8.attention.self.query.bias обучаемый: False\n",
            "Параметр bert.encoder.layer.8.attention.self.key.weight обучаемый: False\n",
            "Параметр bert.encoder.layer.8.attention.self.key.bias обучаемый: False\n",
            "Параметр bert.encoder.layer.8.attention.self.value.weight обучаемый: False\n",
            "Параметр bert.encoder.layer.8.attention.self.value.bias обучаемый: False\n",
            "Параметр bert.encoder.layer.8.attention.output.dense.weight обучаемый: False\n",
            "Параметр bert.encoder.layer.8.attention.output.dense.bias обучаемый: False\n",
            "Параметр bert.encoder.layer.8.attention.output.LayerNorm.weight обучаемый: False\n",
            "Параметр bert.encoder.layer.8.attention.output.LayerNorm.bias обучаемый: False\n",
            "Параметр bert.encoder.layer.8.intermediate.dense.weight обучаемый: False\n",
            "Параметр bert.encoder.layer.8.intermediate.dense.bias обучаемый: False\n",
            "Параметр bert.encoder.layer.8.output.dense.weight обучаемый: False\n",
            "Параметр bert.encoder.layer.8.output.dense.bias обучаемый: False\n",
            "Параметр bert.encoder.layer.8.output.LayerNorm.weight обучаемый: False\n",
            "Параметр bert.encoder.layer.8.output.LayerNorm.bias обучаемый: False\n",
            "Параметр bert.encoder.layer.9.attention.self.query.weight обучаемый: False\n",
            "Параметр bert.encoder.layer.9.attention.self.query.bias обучаемый: False\n",
            "Параметр bert.encoder.layer.9.attention.self.key.weight обучаемый: False\n",
            "Параметр bert.encoder.layer.9.attention.self.key.bias обучаемый: False\n",
            "Параметр bert.encoder.layer.9.attention.self.value.weight обучаемый: False\n",
            "Параметр bert.encoder.layer.9.attention.self.value.bias обучаемый: False\n",
            "Параметр bert.encoder.layer.9.attention.output.dense.weight обучаемый: False\n",
            "Параметр bert.encoder.layer.9.attention.output.dense.bias обучаемый: False\n",
            "Параметр bert.encoder.layer.9.attention.output.LayerNorm.weight обучаемый: False\n",
            "Параметр bert.encoder.layer.9.attention.output.LayerNorm.bias обучаемый: False\n",
            "Параметр bert.encoder.layer.9.intermediate.dense.weight обучаемый: False\n",
            "Параметр bert.encoder.layer.9.intermediate.dense.bias обучаемый: False\n",
            "Параметр bert.encoder.layer.9.output.dense.weight обучаемый: False\n",
            "Параметр bert.encoder.layer.9.output.dense.bias обучаемый: False\n",
            "Параметр bert.encoder.layer.9.output.LayerNorm.weight обучаемый: False\n",
            "Параметр bert.encoder.layer.9.output.LayerNorm.bias обучаемый: False\n",
            "Параметр bert.encoder.layer.10.attention.self.query.weight обучаемый: False\n",
            "Параметр bert.encoder.layer.10.attention.self.query.bias обучаемый: False\n",
            "Параметр bert.encoder.layer.10.attention.self.key.weight обучаемый: False\n",
            "Параметр bert.encoder.layer.10.attention.self.key.bias обучаемый: False\n",
            "Параметр bert.encoder.layer.10.attention.self.value.weight обучаемый: False\n",
            "Параметр bert.encoder.layer.10.attention.self.value.bias обучаемый: False\n",
            "Параметр bert.encoder.layer.10.attention.output.dense.weight обучаемый: False\n",
            "Параметр bert.encoder.layer.10.attention.output.dense.bias обучаемый: False\n",
            "Параметр bert.encoder.layer.10.attention.output.LayerNorm.weight обучаемый: False\n",
            "Параметр bert.encoder.layer.10.attention.output.LayerNorm.bias обучаемый: False\n",
            "Параметр bert.encoder.layer.10.intermediate.dense.weight обучаемый: False\n",
            "Параметр bert.encoder.layer.10.intermediate.dense.bias обучаемый: False\n",
            "Параметр bert.encoder.layer.10.output.dense.weight обучаемый: False\n",
            "Параметр bert.encoder.layer.10.output.dense.bias обучаемый: False\n",
            "Параметр bert.encoder.layer.10.output.LayerNorm.weight обучаемый: False\n",
            "Параметр bert.encoder.layer.10.output.LayerNorm.bias обучаемый: False\n",
            "Параметр bert.encoder.layer.11.attention.self.query.weight обучаемый: False\n",
            "Параметр bert.encoder.layer.11.attention.self.query.bias обучаемый: False\n",
            "Параметр bert.encoder.layer.11.attention.self.key.weight обучаемый: False\n",
            "Параметр bert.encoder.layer.11.attention.self.key.bias обучаемый: False\n",
            "Параметр bert.encoder.layer.11.attention.self.value.weight обучаемый: False\n",
            "Параметр bert.encoder.layer.11.attention.self.value.bias обучаемый: False\n",
            "Параметр bert.encoder.layer.11.attention.output.dense.weight обучаемый: False\n",
            "Параметр bert.encoder.layer.11.attention.output.dense.bias обучаемый: False\n",
            "Параметр bert.encoder.layer.11.attention.output.LayerNorm.weight обучаемый: False\n",
            "Параметр bert.encoder.layer.11.attention.output.LayerNorm.bias обучаемый: False\n",
            "Параметр bert.encoder.layer.11.intermediate.dense.weight обучаемый: False\n",
            "Параметр bert.encoder.layer.11.intermediate.dense.bias обучаемый: False\n",
            "Параметр bert.encoder.layer.11.output.dense.weight обучаемый: False\n",
            "Параметр bert.encoder.layer.11.output.dense.bias обучаемый: False\n",
            "Параметр bert.encoder.layer.11.output.LayerNorm.weight обучаемый: False\n",
            "Параметр bert.encoder.layer.11.output.LayerNorm.bias обучаемый: False\n",
            "Параметр bert.pooler.dense.weight обучаемый: False\n",
            "Параметр bert.pooler.dense.bias обучаемый: False\n",
            "Параметр classifier.1.weight обучаемый: True\n",
            "Параметр classifier.1.bias обучаемый: True\n",
            "Параметр classifier.3.weight обучаемый: True\n",
            "Параметр classifier.3.bias обучаемый: True\n",
            "Параметр classifier.5.weight обучаемый: True\n",
            "Параметр classifier.5.bias обучаемый: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda'\n",
        "model.to(device)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "b5q3YZl_1gNN",
        "outputId": "c700b873-24d6-4d14-eb23-37339eb0761d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertClassifier1(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSdpaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (classifier): Sequential(\n",
              "    (0): Dropout(p=0.3, inplace=False)\n",
              "    (1): Linear(in_features=768, out_features=256, bias=True)\n",
              "    (2): ReLU()\n",
              "    (3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (4): Dropout(p=0.2, inplace=False)\n",
              "    (5): Linear(in_features=256, out_features=1, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Пример текста:\", texts_train[0])\n",
        "print(\"Токенизированный:\", tokenizer.tokenize(texts_train[0]))"
      ],
      "metadata": {
        "id": "8O1x5fMWBC4P",
        "outputId": "0f674389-f878-4afe-9eb2-3d61073145c5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Пример текста: a stirring , funny and finally transporting re imagining of beauty and the beast and 1930s horror films\n",
            "Токенизированный: ['a', 'stirring', ',', 'funny', 'and', 'finally', 'transporting', 're', 'imagining', 'of', 'beauty', 'and', 'the', 'beast', 'and', '1930s', 'horror', 'films']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=3e-5)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "train(model, optimizer, criterion, train_dl, test_dl)"
      ],
      "metadata": {
        "id": "BH7ehe9a1ijw",
        "outputId": "2783aa3f-b6e5-4b13-ab74-4bfc600e25d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 40/40 [00:11<00:00,  3.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 0.9080865430831909 0.00395625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing: 100%|██████████| 15/15 [00:04<00:00,  3.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.6855356335639954\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 40/40 [00:11<00:00,  3.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 0.6767085948944092 0.0041328125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing: 100%|██████████| 15/15 [00:04<00:00,  3.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.6643527905146281\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 40/40 [00:12<00:00,  3.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 0.6597808517456055 0.004475\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing: 100%|██████████| 15/15 [00:04<00:00,  3.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.6519868850708008\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 40/40 [00:12<00:00,  3.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 0.6459972919464111 0.0047125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing: 100%|██████████| 15/15 [00:04<00:00,  3.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.6393607934315999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 40/40 [00:11<00:00,  3.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 0.6343007710456848 0.0047640625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing: 100%|██████████| 15/15 [00:04<00:00,  3.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.629047421614329\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 40/40 [00:11<00:00,  3.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 0.625011132144928 0.0049890625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing: 100%|██████████| 15/15 [00:04<00:00,  3.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.6200115323066712\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 40/40 [00:11<00:00,  3.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 0.6152362343788147 0.0050875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing: 100%|██████████| 15/15 [00:04<00:00,  3.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.6132086992263794\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 40/40 [00:11<00:00,  3.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 0.6098324132919312 0.0049484375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing: 100%|██████████| 15/15 [00:04<00:00,  3.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.6063524087270101\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 40/40 [00:11<00:00,  3.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 0.6016217460632324 0.005246875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing: 100%|██████████| 15/15 [00:04<00:00,  3.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.6030762632687886\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 40/40 [00:11<00:00,  3.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 0.5963711725234986 0.0052546875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing: 100%|██████████| 15/15 [00:04<00:00,  3.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.5963854193687439\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Дробавим классифкатор на выход модели"
      ],
      "metadata": {
        "id": "yaNayu-zgYyg"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMM-itSbGWkX"
      },
      "source": [
        "#### Сдача взадания в контест\n",
        "Сохраните в словарь `out_dict` вероятности принадлежности к первому (положительному) классу"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fZGSItlzGWkX"
      },
      "outputs": [],
      "source": [
        "out_dict = {\n",
        "    'train': # list of length 5000 with probas\n",
        "    'test': # list of length 1920 with probas\n",
        "    'holdout': # list of length 500 with probas\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4pSCUFd-GWkX"
      },
      "source": [
        "Несколько `assert`'ов для проверки вашей посылки:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L5P530fnGWkX"
      },
      "outputs": [],
      "source": [
        "assert isinstance(out_dict[\"train\"], list), \"Object must be a list of floats\"\n",
        "assert isinstance(out_dict[\"train\"][0], float), \"Object must be a list of floats\"\n",
        "assert (\n",
        "    len(out_dict[\"train\"]) == 5000\n",
        "), \"The predicted probas list length does not match the train set size\"\n",
        "\n",
        "assert isinstance(out_dict[\"test\"], list), \"Object must be a list of floats\"\n",
        "assert isinstance(out_dict[\"test\"][0], float), \"Object must be a list of floats\"\n",
        "assert (\n",
        "    len(out_dict[\"test\"]) == 1920\n",
        "), \"The predicted probas list length does not match the test set size\"\n",
        "\n",
        "assert isinstance(out_dict[\"holdout\"], list), \"Object must be a list of floats\"\n",
        "assert isinstance(out_dict[\"holdout\"][0], float), \"Object must be a list of floats\"\n",
        "assert len(\n",
        "    out_dict[\"holdout\"] == 500\n",
        "), \"The predicted probas list length does not match the holdout set size\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TuLFV72BGWkX"
      },
      "source": [
        "Запустите код ниже для генерации посылки."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RR5WOzKAGWkX"
      },
      "outputs": [],
      "source": [
        "# do not change the code in the block below\n",
        "# __________start of block__________\n",
        "FILENAME = \"submission_dict_hw_text_classification_with_bert.json\"\n",
        "\n",
        "with open(FILENAME, \"w\") as iofile:\n",
        "    json.dump(out_dict, iofile)\n",
        "print(f\"File saved to `{FILENAME}`\")\n",
        "# __________end of block__________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQ2gSCi3GWkX"
      },
      "source": [
        "На этом задание завершено. Поздравляем!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "NMM-itSbGWkX"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}