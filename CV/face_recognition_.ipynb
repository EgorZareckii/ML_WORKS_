{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Face Recognition\n"
      ],
      "metadata": {
        "id": "156L-QwUnq7E"
      },
      "id": "156L-QwUnq7E"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Описание задачи**: В этой задаче требуется сделать модель распознавания лиц на основе фотографий людей. В ходе работы будут рассмотрены несколько моделей и несколько подходов к решению этой задачи."
      ],
      "metadata": {
        "id": "vLxIqSnNn2Zc"
      },
      "id": "vLxIqSnNn2Zc"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "45a24e4d",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-21T22:22:49.815914Z",
          "iopub.status.busy": "2025-04-21T22:22:49.815454Z",
          "iopub.status.idle": "2025-04-21T22:23:01.361979Z",
          "shell.execute_reply": "2025-04-21T22:23:01.361046Z"
        },
        "papermill": {
          "duration": 11.759529,
          "end_time": "2025-04-21T22:23:01.363694",
          "exception": false,
          "start_time": "2025-04-21T22:22:49.604165",
          "status": "completed"
        },
        "tags": [],
        "id": "45a24e4d"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "from torchvision import datasets, models, transforms\n",
        "import numpy as np\n",
        "import os\n",
        "from pathlib import Path\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torchvision.datasets import ImageFolder\n",
        "import os\n",
        "from torch.utils.data import Subset\n",
        "from itertools import combinations\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "b043077e",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "collapsed": true,
        "execution": {
          "iopub.execute_input": "2025-04-21T22:19:14.845600Z",
          "iopub.status.busy": "2025-04-21T22:19:14.845380Z",
          "iopub.status.idle": "2025-04-21T22:22:49.390481Z",
          "shell.execute_reply": "2025-04-21T22:22:49.389270Z"
        },
        "jupyter": {
          "outputs_hidden": true
        },
        "papermill": {
          "duration": 214.551657,
          "end_time": "2025-04-21T22:22:49.391864",
          "exception": false,
          "start_time": "2025-04-21T22:19:14.840207",
          "status": "completed"
        },
        "tags": [],
        "id": "b043077e"
      },
      "outputs": [],
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "В качестве датасета для решения этой задачи я буду использовать довольно популярный датасет для распознавания лиц: LFW Dataset. Его я взял с Kaggle\n",
        "\n",
        "**Описание датасета**: в датасете представлены папки с фотографиями людей для каждого человека. У каждого человека 2< фотографий."
      ],
      "metadata": {
        "id": "ZENByiTSoYd8"
      },
      "id": "ZENByiTSoYd8"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "0f2eb258",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-21T22:23:01.751633Z",
          "iopub.status.busy": "2025-04-21T22:23:01.751158Z",
          "iopub.status.idle": "2025-04-21T22:23:02.344621Z",
          "shell.execute_reply": "2025-04-21T22:23:02.343822Z"
        },
        "papermill": {
          "duration": 0.788011,
          "end_time": "2025-04-21T22:23:02.345910",
          "exception": false,
          "start_time": "2025-04-21T22:23:01.557899",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0f2eb258",
        "outputId": "8fc9ad20-1983-449c-e188-248e3a726e4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /kaggle/input/lfw-dataset\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"jessicali9530/lfw-dataset\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Чтобы не было утечек, надо сначала разбить папки на трейн и тест и потом делить на пары"
      ],
      "metadata": {
        "id": "p1cYuo0DrgPD"
      },
      "id": "p1cYuo0DrgPD"
    },
    {
      "cell_type": "code",
      "source": [
        "folders = [name for name in os.listdir('/kaggle/input/lfw-dataset/lfw-deepfunneled/lfw-deepfunneled')]\n",
        "train_classes, test_classes = train_test_split(\n",
        "    folders,\n",
        "    test_size=0.2,\n",
        "    random_state=40,\n",
        "    shuffle=True\n",
        ")"
      ],
      "metadata": {
        "id": "A4MLSOtjrIoY"
      },
      "id": "A4MLSOtjrIoY",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "5179dbb0",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-21T22:23:03.125116Z",
          "iopub.status.busy": "2025-04-21T22:23:03.124772Z",
          "iopub.status.idle": "2025-04-21T22:23:10.371325Z",
          "shell.execute_reply": "2025-04-21T22:23:10.370325Z"
        },
        "papermill": {
          "duration": 7.441036,
          "end_time": "2025-04-21T22:23:10.372680",
          "exception": false,
          "start_time": "2025-04-21T22:23:02.931644",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5179dbb0",
        "outputId": "fdc9b9cc-5b7d-48a1-b97f-0afedcdf3286"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of images:  13233\n"
          ]
        }
      ],
      "source": [
        "folders = sorted(folders)\n",
        "path = '/kaggle/input/lfw-dataset/lfw-deepfunneled/lfw-deepfunneled'\n",
        "images_num = 0\n",
        "for name in folders:\n",
        "  files = os.listdir(f'/kaggle/input/lfw-dataset/lfw-deepfunneled/lfw-deepfunneled/{name}')\n",
        "  image_files = [f for f in files if f.lower().endswith('.jpg')]\n",
        "  images_num += len(image_files)\n",
        "\n",
        "print(\"Number of images: \", images_num)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Дальнейшее решение предполагает использование **contrastive learning** для обучени модели, поэтому я разобью изображения лиц положительные и негативные пары: положительные - один человек => метка 1, негативные - разные люди => метка 0."
      ],
      "metadata": {
        "id": "5YR_GiQSosKm"
      },
      "id": "5YR_GiQSosKm"
    },
    {
      "cell_type": "markdown",
      "source": [
        "ТРЕЙН ПАРЫ"
      ],
      "metadata": {
        "id": "nug84dVYrxl-"
      },
      "id": "nug84dVYrxl-"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "29f5ffbe",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-21T22:23:10.767214Z",
          "iopub.status.busy": "2025-04-21T22:23:10.766869Z",
          "iopub.status.idle": "2025-04-21T22:23:18.723048Z",
          "shell.execute_reply": "2025-04-21T22:23:18.722024Z"
        },
        "papermill": {
          "duration": 8.149305,
          "end_time": "2025-04-21T22:23:18.724551",
          "exception": false,
          "start_time": "2025-04-21T22:23:10.575246",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29f5ffbe",
        "outputId": "eb9db709-7bb5-430e-e3ec-0a9cb7ca4826"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✔ Создано 230017 позитивных тренировочных пар и 230017 негативных пар.\n"
          ]
        }
      ],
      "source": [
        "dataset_path = \"/kaggle/input/lfw-dataset/lfw-deepfunneled/lfw-deepfunneled\"\n",
        "\n",
        "people_images = {person: os.listdir(os.path.join(dataset_path, person))\n",
        "                 for person in train_classes\n",
        "                 if os.path.isdir(os.path.join(dataset_path, person))}\n",
        "\n",
        "positive_pairs = []\n",
        "negative_pairs = []\n",
        "\n",
        "for person, images in people_images.items():\n",
        "    if len(images) > 1:\n",
        "        positive_pairs += [(i,1) for i in combinations(images, 2)]\n",
        "\n",
        "people = list(people_images.keys())\n",
        "for _ in range(len(positive_pairs)):\n",
        "    p1, p2 = random.sample(people, 2)\n",
        "    img1 = random.choice(people_images[p1])\n",
        "    img2 = random.choice(people_images[p2])\n",
        "    negative_pairs.append(((img1, img2),-1))\n",
        "\n",
        "print(f\"✔ Создано {len(positive_pairs)} позитивных тренировочных пар и {len(negative_pairs)} негативных пар.\")\n",
        "pairs_train = positive_pairs + negative_pairs"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ТЕСТ ПАРЫ"
      ],
      "metadata": {
        "id": "CDKtnJcbr8GX"
      },
      "id": "CDKtnJcbr8GX"
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = \"/kaggle/input/lfw-dataset/lfw-deepfunneled/lfw-deepfunneled\"\n",
        "\n",
        "# Собираем список людей и их изображений\n",
        "people_images = {person: os.listdir(os.path.join(dataset_path, person))\n",
        "                 for person in test_classes\n",
        "                 if os.path.isdir(os.path.join(dataset_path, person))}\n",
        "\n",
        "positive_pairs = []\n",
        "negative_pairs = []\n",
        "\n",
        "for person, images in people_images.items():\n",
        "    if len(images) > 1:\n",
        "        positive_pairs += [(i,1) for i in combinations(images, 2)]\n",
        "\n",
        "people = list(people_images.keys())\n",
        "for _ in range(len(positive_pairs)):\n",
        "    p1, p2 = random.sample(people, 2)\n",
        "    img1 = random.choice(people_images[p1])\n",
        "    img2 = random.choice(people_images[p2])\n",
        "    negative_pairs.append(((img1, img2),-1))\n",
        "\n",
        "print(f\"✔ Создано {len(positive_pairs)} позитивных тестовых пар и {len(negative_pairs)} негативных пар.\")\n",
        "pairs_test = positive_pairs + negative_pairs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-ERldnJr2jJ",
        "outputId": "61514bb5-6bcf-4691-f22f-d6e9ebad1896"
      },
      "id": "0-ERldnJr2jJ",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✔ Создано 12240 позитивных тестовых пар и 12240 негативных пар.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b685031d",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-21T22:23:19.527272Z",
          "iopub.status.busy": "2025-04-21T22:23:19.526679Z",
          "iopub.status.idle": "2025-04-21T22:23:19.580293Z",
          "shell.execute_reply": "2025-04-21T22:23:19.579533Z"
        },
        "papermill": {
          "duration": 0.240483,
          "end_time": "2025-04-21T22:23:19.581610",
          "exception": false,
          "start_time": "2025-04-21T22:23:19.341127",
          "status": "completed"
        },
        "tags": [],
        "id": "b685031d",
        "outputId": "9df38e08-315e-4492-e013-9081e20d4c2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(250, 250)\n"
          ]
        }
      ],
      "source": [
        "n = Image.open('/kaggle/input/lfw-dataset/lfw-deepfunneled/lfw-deepfunneled/AJ_Cook/AJ_Cook_0001.jpg')\n",
        "print(n.size)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для дальнейшего обучения модели напишем кастомный датасет"
      ],
      "metadata": {
        "id": "5TXMBL42tATr"
      },
      "id": "5TXMBL42tATr"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "7cebeb17",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-21T22:23:19.964541Z",
          "iopub.status.busy": "2025-04-21T22:23:19.964198Z",
          "iopub.status.idle": "2025-04-21T22:23:19.970419Z",
          "shell.execute_reply": "2025-04-21T22:23:19.969755Z"
        },
        "papermill": {
          "duration": 0.201064,
          "end_time": "2025-04-21T22:23:19.971727",
          "exception": false,
          "start_time": "2025-04-21T22:23:19.770663",
          "status": "completed"
        },
        "tags": [],
        "id": "7cebeb17"
      },
      "outputs": [],
      "source": [
        "class FaceEshkereDatasetPair(Dataset):\n",
        "  def __init__(self, pairs, mode):\n",
        "    self.pairs = pairs\n",
        "    self.mode = mode\n",
        "    self.transform_train = transforms.Compose([\n",
        "      transforms.Resize((112, 112)),\n",
        "      transforms.RandomHorizontalFlip(p=0.2),\n",
        "      transforms.ToTensor()\n",
        "      ])\n",
        "    self.transform_test = transforms.Compose([\n",
        "      transforms.Resize((112, 112)),\n",
        "      transforms.ToTensor()\n",
        "      ])\n",
        "  def __len__(self):\n",
        "    return len(self.pairs)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    path_to_image0 = Path(f\"/kaggle/input/lfw-dataset/lfw-deepfunneled/lfw-deepfunneled/{self.pairs[index][0][0][:-9]}/{self.pairs[index][0][0]}\") #надо бы поменять конечно, но работает\n",
        "    path_to_image1 = Path(f\"/kaggle/input/lfw-dataset/lfw-deepfunneled/lfw-deepfunneled/{self.pairs[index][0][1][:-9]}/{self.pairs[index][0][1]}\")\n",
        "\n",
        "    label = self.pairs[index][1]\n",
        "\n",
        "    if self.mode == 'train':\n",
        "      image0 = self.transform_train(Image.open(path_to_image0))\n",
        "      image1 = self.transform_train(Image.open(path_to_image1))\n",
        "    if self.mode == 'test':\n",
        "      image0 = self.transform_test(Image.open(path_to_image0))\n",
        "      image1 = self.transform_test(Image.open(path_to_image1))\n",
        "\n",
        "    return image0, image1, label\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "1063c7ec",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-21T22:23:23.415556Z",
          "iopub.status.busy": "2025-04-21T22:23:23.414674Z",
          "iopub.status.idle": "2025-04-21T22:23:23.422140Z",
          "shell.execute_reply": "2025-04-21T22:23:23.421270Z"
        },
        "papermill": {
          "duration": 0.195579,
          "end_time": "2025-04-21T22:23:23.423410",
          "exception": false,
          "start_time": "2025-04-21T22:23:23.227831",
          "status": "completed"
        },
        "tags": [],
        "id": "1063c7ec"
      },
      "outputs": [],
      "source": [
        "test_dataset = FaceEshkereDatasetPair(pairs_test, 'test')\n",
        "train_dataset = FaceEshkereDatasetPair(pairs_train, 'train')\n",
        "\n",
        "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=2, pin_memory=True)\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=2, pin_memory=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "7e9fc8b4",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-21T22:23:23.804865Z",
          "iopub.status.busy": "2025-04-21T22:23:23.804353Z",
          "iopub.status.idle": "2025-04-21T22:23:23.811857Z",
          "shell.execute_reply": "2025-04-21T22:23:23.811050Z"
        },
        "papermill": {
          "duration": 0.204773,
          "end_time": "2025-04-21T22:23:23.813142",
          "exception": false,
          "start_time": "2025-04-21T22:23:23.608369",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7e9fc8b4",
        "outputId": "5365dc35-e328-47b2-8256-5f07acb34cc4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "460034"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "len(train_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Модели"
      ],
      "metadata": {
        "id": "HPppmWWUwDad"
      },
      "id": "HPppmWWUwDad"
    },
    {
      "cell_type": "markdown",
      "source": [
        "В моем решении я буду рассматривать несколько моделей для решения этой задачи:\n",
        "\n",
        "Resnet 18 - сверточная нейросеть с маленьким числом параметров.\n",
        "VIT - визуальный трансформер, который использует внимание вместо сверток"
      ],
      "metadata": {
        "id": "gtOnJYUAwGo_"
      },
      "id": "gtOnJYUAwGo_"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Рассмотрим и обучим Resnet18\n",
        "\n",
        "Мой выбор пал именно на реснет18 из-за его компактной архитектуры, так как эта задача требует больших ресурсов для  вычислительных мощностей которыми я располагаю. Благодаря не такому большрому числу параметров можно обучить его *больше*"
      ],
      "metadata": {
        "id": "n3mlJAl_wcSe"
      },
      "id": "n3mlJAl_wcSe"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "73a6bb08",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-21T22:23:24.201126Z",
          "iopub.status.busy": "2025-04-21T22:23:24.200626Z",
          "iopub.status.idle": "2025-04-21T22:23:24.945039Z",
          "shell.execute_reply": "2025-04-21T22:23:24.944316Z"
        },
        "papermill": {
          "duration": 0.941008,
          "end_time": "2025-04-21T22:23:24.946452",
          "exception": false,
          "start_time": "2025-04-21T22:23:24.005444",
          "status": "completed"
        },
        "tags": [],
        "id": "73a6bb08"
      },
      "outputs": [],
      "source": [
        "from torchvision.models import resnet18\n",
        "from tqdm import tqdm, tqdm_notebook\n",
        "\n",
        "model = resnet18(pretrained=True)\n",
        "\n",
        "for param in model.parameters():\n",
        "  param.requires_grad = True\n",
        "model.fc = nn.Linear(512,128, bias=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Загрузка весов (этот этап кода нужен уже после обучения для теста модели)"
      ],
      "metadata": {
        "id": "vfNbzSfyycP-"
      },
      "id": "vfNbzSfyycP-"
    },
    {
      "cell_type": "code",
      "source": [
        "w = torch.load('/content/sample_data/model_weights_epoch_9.pth',map_location=torch.device('cpu') )\n",
        "model.load_state_dict(w)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9QpkmQMCx6hC",
        "outputId": "3b49929c-4030-439a-8152-e5a1fab61f23"
      },
      "id": "9QpkmQMCx6hC",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Функция с тренировкой модели и валидацией"
      ],
      "metadata": {
        "id": "qK3bi5DQylO9"
      },
      "id": "qK3bi5DQylO9"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "42a7303e",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-21T22:23:25.322088Z",
          "iopub.status.busy": "2025-04-21T22:23:25.321527Z",
          "iopub.status.idle": "2025-04-21T22:23:25.336566Z",
          "shell.execute_reply": "2025-04-21T22:23:25.335705Z"
        },
        "papermill": {
          "duration": 0.203696,
          "end_time": "2025-04-21T22:23:25.337986",
          "exception": false,
          "start_time": "2025-04-21T22:23:25.134290",
          "status": "completed"
        },
        "tags": [],
        "id": "42a7303e"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "from torch.nn import DataParallel\n",
        "\n",
        "def train(model, optimizer, criterion, epochs, train_dataloader, test_dataloader):\n",
        "    summary_loss_train = []\n",
        "    summary_loss_test = []\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    if torch.cuda.device_count() > 1:\n",
        "        print(f\"Using {torch.cuda.device_count()} GPUs\") # штука для 2 и более гпу\n",
        "        model = DataParallel(model)\n",
        "\n",
        "    for epoch in tqdm(range(epochs), desc=\"Epochs\", ncols=100):\n",
        "        train_loss = 0\n",
        "        train_norm_variable = 0\n",
        "        model.train()\n",
        "        for image0, image1, label in train_dataloader:\n",
        "            image0 = image0.to(device)\n",
        "            image1 = image1.to(device)\n",
        "            label = label.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            embed0 = model(image0)\n",
        "            embed1 = model(image1)\n",
        "\n",
        "            loss = criterion(embed0, embed1, label)\n",
        "            summary_loss_train.append(loss.item())\n",
        "            train_loss += loss.item() * image0.size(0)\n",
        "            train_norm_variable += image0.size(0)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        print(f'Train loss: {train_loss/train_norm_variable}')\n",
        "\n",
        "        torch.save(model.module.state_dict() if isinstance(model, DataParallel) else model.state_dict(),\n",
        "                   f'/kaggle/working/model_weights_epoch_{epoch + 14}.pth')\n",
        "        print(f\"Weights saved for epoch {epoch + 14}\")\n",
        "\n",
        "        test_loss = 0\n",
        "        test_norm_variable = 0\n",
        "        model.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for image0, image1, label in test_dataloader:\n",
        "                image0 = image0.to(device)\n",
        "                image1 = image1.to(device)\n",
        "                label = label.to(device)\n",
        "\n",
        "                embed0 = model(image0)\n",
        "                embed1 = model(image1)\n",
        "                loss = criterion(embed0, embed1, label)\n",
        "                summary_loss_test.append(loss.item())\n",
        "\n",
        "                test_loss += loss.item() * image0.size(0)\n",
        "                test_norm_variable += image0.size(0)\n",
        "\n",
        "        print(f'Test loss: {test_loss/test_norm_variable}')\n",
        "    return summary_loss_train, summary_loss_test"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "В качестве функции потерь я буду использовать **CosineEmbeddingLoss** из torch. Это как раз таки пример контрастного обучения на парах, где мы учим модель в векторном пространстве в котором у эмбедингов одних лиц косинусное расстояние больше (угол меньше), а у разных лиц наоборот.\n",
        "Так же активно используются **TripletLoss** **ArcFaceLoss** и другие. Триплетный лосс похож на тот который я использую но только на одной итерации ошибка складывается сразу как с негативной так и с положительной парой а не поочередно."
      ],
      "metadata": {
        "id": "4ufoCk_x5R_T"
      },
      "id": "4ufoCk_x5R_T"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "6a58ece8",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-21T22:23:25.751599Z",
          "iopub.status.busy": "2025-04-21T22:23:25.751027Z",
          "iopub.status.idle": "2025-04-21T22:23:25.756949Z",
          "shell.execute_reply": "2025-04-21T22:23:25.756297Z"
        },
        "papermill": {
          "duration": 0.215029,
          "end_time": "2025-04-21T22:23:25.758306",
          "exception": false,
          "start_time": "2025-04-21T22:23:25.543277",
          "status": "completed"
        },
        "tags": [],
        "id": "6a58ece8"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=3e-4)\n",
        "criterion = nn.CosineEmbeddingLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4417648",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-21T22:23:26.168295Z",
          "iopub.status.busy": "2025-04-21T22:23:26.167942Z",
          "iopub.status.idle": "2025-04-22T00:20:14.337129Z",
          "shell.execute_reply": "2025-04-22T00:20:14.336192Z"
        },
        "papermill": {
          "duration": 7008.550415,
          "end_time": "2025-04-22T00:20:14.518131",
          "exception": false,
          "start_time": "2025-04-21T22:23:25.967716",
          "status": "completed"
        },
        "tags": [],
        "id": "f4417648",
        "outputId": "a4e2a93a-c9f9-470e-c566-9837cbff710d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using 2 GPUs\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epochs:   0%|                                                                | 0/15 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train loss: 0.0010565760603973883\n",
            "Weights saved for epoch 14\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epochs:   7%|███▌                                                 | 1/15 [07:33<1:45:44, 453.15s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test loss: 1.034893308367048e-06\n",
            "Train loss: 6.940654346180964e-07\n",
            "Weights saved for epoch 15\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epochs:  13%|███████                                              | 2/15 [15:24<1:40:32, 464.03s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test loss: 4.830871309877694e-07\n",
            "Train loss: 3.7060720582563333e-07\n",
            "Weights saved for epoch 16\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epochs:  20%|██████████▌                                          | 3/15 [23:14<1:33:21, 466.80s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test loss: 2.865408148084368e-07\n",
            "Train loss: 2.2716096471283112e-07\n",
            "Weights saved for epoch 17\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epochs:  27%|██████████████▏                                      | 4/15 [30:55<1:25:09, 464.47s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test loss: 1.7931035588065113e-07\n",
            "Train loss: 1.4693140984231183e-07\n",
            "Weights saved for epoch 18\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epochs:  33%|█████████████████▋                                   | 5/15 [38:43<1:17:37, 465.73s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test loss: 1.1809212821844994e-07\n",
            "Train loss: 9.943672589380575e-08\n",
            "Weights saved for epoch 19\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epochs:  40%|█████████████████████▏                               | 6/15 [46:29<1:09:51, 465.67s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test loss: 8.290239743148829e-08\n",
            "Train loss: 7.12820461834391e-08\n",
            "Weights saved for epoch 20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epochs:  47%|████████████████████████▋                            | 7/15 [54:28<1:02:40, 470.00s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test loss: 6.065538951653642e-08\n",
            "Train loss: 5.1850931987473816e-08\n",
            "Weights saved for epoch 21\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epochs:  53%|████████████████████████████▎                        | 8/15 [1:02:30<55:17, 473.88s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test loss: 4.406571388650653e-08\n",
            "Train loss: 3.83547374212867e-08\n",
            "Weights saved for epoch 22\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epochs:  60%|███████████████████████████████▊                     | 9/15 [1:10:07<46:52, 468.68s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test loss: 3.187230655206414e-08\n",
            "Train loss: 2.7464968820416807e-08\n",
            "Weights saved for epoch 23\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epochs:  67%|██████████████████████████████████▋                 | 10/15 [1:18:02<39:12, 470.57s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test loss: 2.210140228352689e-08\n",
            "Train loss: 1.994456563920721e-08\n",
            "Weights saved for epoch 24\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epochs:  73%|██████████████████████████████████████▏             | 11/15 [1:25:39<31:05, 466.45s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test loss: 1.584717205836179e-08\n",
            "Train loss: 1.351833343566763e-08\n",
            "Weights saved for epoch 25\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epochs:  80%|█████████████████████████████████████████▌          | 12/15 [1:33:31<23:24, 468.03s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test loss: 9.632962091440537e-09\n",
            "Train loss: 8.468968528266682e-09\n",
            "Weights saved for epoch 26\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epochs:  87%|█████████████████████████████████████████████       | 13/15 [1:41:21<15:37, 468.81s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test loss: 5.931513651091791e-09\n",
            "Train loss: 4.027570996864174e-09\n",
            "Weights saved for epoch 27\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epochs:  93%|████████████████████████████████████████████████▌   | 14/15 [1:49:13<07:49, 469.63s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test loss: 1.3973031717081799e-09\n",
            "Train loss: 7.952962600857713e-10\n",
            "Weights saved for epoch 28\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epochs: 100%|████████████████████████████████████████████████████| 15/15 [1:56:47<00:00, 467.19s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test loss: -1.1605875832313269e-09\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "([0.4927073121070862,\n",
              "  0.07531451433897018,\n",
              "  0.006135598290711641,\n",
              "  0.0010899826884269714,\n",
              "  0.0004211678169667721,\n",
              "  0.00019638286903500557,\n",
              "  0.00010436214506626129,\n",
              "  6.893370300531387e-05,\n",
              "  6.0974154621362686e-05,\n",
              "  4.7711655497550964e-05,\n",
              "  3.4275464713573456e-05,\n",
              "  3.372738137841225e-05,\n",
              "  2.6932917535305023e-05,\n",
              "  2.58483923971653e-05,\n",
              "  2.366025000810623e-05,\n",
              "  2.2020190954208374e-05,\n",
              "  2.0219944417476654e-05,\n",
              "  1.849886029958725e-05,\n",
              "  2.0417850464582443e-05,\n",
              "  1.6530510038137436e-05,\n",
              "  1.7116311937570572e-05,\n",
              "  1.5332363545894623e-05,\n",
              "  1.5101395547389984e-05,\n",
              "  1.3976357877254486e-05,\n",
              "  1.2835022062063217e-05,\n",
              "  1.4459248632192612e-05,\n",
              "  1.3607088476419449e-05,\n",
              "  1.2668780982494354e-05,\n",
              "  1.2064352631568909e-05,\n",
              "  1.3669021427631378e-05,\n",
              "  9.695533663034439e-06,\n",
              "  1.2082047760486603e-05,\n",
              "  1.1049211025238037e-05,\n",
              "  1.0640360414981842e-05,\n",
              "  9.4524584710598e-06,\n",
              "  1.0329298675060272e-05,\n",
              "  9.403098374605179e-06,\n",
              "  9.443610906600952e-06,\n",
              "  9.323004633188248e-06,\n",
              "  9.084120392799377e-06,\n",
              "  8.594710379838943e-06,\n",
              "  8.518807590007782e-06,\n",
              "  8.5378997027874e-06,\n",
              "  8.36281105875969e-06,\n",
              "  8.186325430870056e-06,\n",
              "  7.692258805036545e-06,\n",
              "  8.6221843957901e-06,\n",
              "  7.503200322389603e-06,\n",
              "  7.259659469127655e-06,\n",
              "  7.146969437599182e-06,\n",
              "  7.951632142066956e-06,\n",
              "  7.045920938253403e-06,\n",
              "  7.14324414730072e-06,\n",
              "  7.009133696556091e-06,\n",
              "  6.372574716806412e-06,\n",
              "  6.4820051193237305e-06,\n",
              "  6.570480763912201e-06,\n",
              "  6.3283368945121765e-06,\n",
              "  6.712973117828369e-06,\n",
              "  6.6980719566345215e-06,\n",
              "  6.430316716432571e-06,\n",
              "  6.162095814943314e-06,\n",
              "  6.029848009347916e-06,\n",
              "  5.9478916227817535e-06,\n",
              "  5.3085386753082275e-06,\n",
              "  5.282461643218994e-06,\n",
              "  5.45009970664978e-06,\n",
              "  5.472451448440552e-06,\n",
              "  5.5516138672828674e-06,\n",
              "  5.6587159633636475e-06,\n",
              "  5.0389207899570465e-06,\n",
              "  5.173496901988983e-06,\n",
              "  5.214940756559372e-06,\n",
              "  5.005393177270889e-06,\n",
              "  5.082692950963974e-06,\n",
              "  4.366040229797363e-06,\n",
              "  4.8549845814704895e-06,\n",
              "  4.745088517665863e-06,\n",
              "  4.815403372049332e-06,\n",
              "  5.526933819055557e-06,\n",
              "  4.366040229797363e-06,\n",
              "  4.791188985109329e-06,\n",
              "  4.825182259082794e-06,\n",
              "  4.592817276716232e-06,\n",
              "  4.929490387439728e-06,\n",
              "  4.051718860864639e-06,\n",
              "  4.302710294723511e-06,\n",
              "  4.524830728769302e-06,\n",
              "  4.017725586891174e-06,\n",
              "  4.275236278772354e-06,\n",
              "  4.189088940620422e-06,\n",
              "  4.454050213098526e-06,\n",
              "  4.406087100505829e-06,\n",
              "  4.147179424762726e-06,\n",
              "  4.105269908905029e-06,\n",
              "  4.144385457038879e-06,\n",
              "  3.7727877497673035e-06,\n",
              "  4.182104021310806e-06,\n",
              "  4.255212843418121e-06,\n",
              "  4.503875970840454e-06,\n",
              "  3.700610250234604e-06,\n",
              "  3.7676654756069183e-06,\n",
              "  3.842171281576157e-06,\n",
              "  4.002824425697327e-06,\n",
              "  3.84356826543808e-06,\n",
              "  4.515983164310455e-06,\n",
              "  4.005618393421173e-06,\n",
              "  3.6368146538734436e-06,\n",
              "  3.528781235218048e-06,\n",
              "  3.6964192986488342e-06,\n",
              "  3.844499588012695e-06,\n",
              "  4.202593117952347e-06,\n",
              "  3.809109330177307e-06,\n",
              "  3.91155481338501e-06,\n",
              "  3.716442734003067e-06,\n",
              "  3.5101547837257385e-06,\n",
              "  3.6242417991161346e-06,\n",
              "  3.0887313187122345e-06,\n",
              "  3.516208380460739e-06,\n",
              "  3.4016557037830353e-06,\n",
              "  3.4295953810214996e-06,\n",
              "  3.5688281059265137e-06,\n",
              "  3.3690594136714935e-06,\n",
              "  3.327149897813797e-06,\n",
              "  4.1476450860500336e-06,\n",
              "  3.430526703596115e-06,\n",
              "  3.3606775104999542e-06,\n",
              "  3.4463591873645782e-06,\n",
              "  3.2191164791584015e-06,\n",
              "  3.205612301826477e-06,\n",
              "  3.1725503504276276e-06,\n",
              "  3.3457763493061066e-06,\n",
              "  3.2032839953899384e-06,\n",
              "  3.38628888130188e-06,\n",
              "  3.3495016396045685e-06,\n",
              "  3.1418167054653168e-06,\n",
              "  2.9043294489383698e-06,\n",
              "  3.2419338822364807e-06,\n",
              "  3.296881914138794e-06,\n",
              "  3.1837262213230133e-06,\n",
              "  3.1674280762672424e-06,\n",
              "  3.0281953513622284e-06,\n",
              "  3.111083060503006e-06,\n",
              "  3.3085234463214874e-06,\n",
              "  3.030523657798767e-06,\n",
              "  3.0319206416606903e-06,\n",
              "  2.9746443033218384e-06,\n",
              "  2.9313378036022186e-06,\n",
              "  2.955552190542221e-06,\n",
              "  3.0901283025741577e-06,\n",
              "  3.2195821404457092e-06,\n",
              "  2.8316862881183624e-06,\n",
              "  2.8852373361587524e-06,\n",
              "  2.96812504529953e-06,\n",
              "  3.0766241252422333e-06,\n",
              "  2.8372742235660553e-06,\n",
              "  2.834014594554901e-06,\n",
              "  2.8209760785102844e-06,\n",
              "  2.6817433536052704e-06,\n",
              "  2.8330832719802856e-06,\n",
              "  2.7706846594810486e-06,\n",
              "  2.842862159013748e-06,\n",
              "  2.871733158826828e-06,\n",
              "  2.989545464515686e-06,\n",
              "  2.636108547449112e-06,\n",
              "  2.814922481775284e-06,\n",
              "  2.743210643529892e-06,\n",
              "  2.874992787837982e-06,\n",
              "  2.7655623853206635e-06,\n",
              "  3.125518560409546e-06,\n",
              "  2.776738256216049e-06,\n",
              "  2.5224871933460236e-06,\n",
              "  2.589542418718338e-06,\n",
              "  2.655666321516037e-06,\n",
              "  2.7241185307502747e-06,\n",
              "  2.7599744498729706e-06,\n",
              "  2.5238841772079468e-06,\n",
              "  2.8191134333610535e-06,\n",
              "  2.5867484509944916e-06,\n",
              "  2.8759241104125977e-06,\n",
              "  2.8135254979133606e-06,\n",
              "  2.6924535632133484e-06,\n",
              "  2.6025809347629547e-06,\n",
              "  2.766493707895279e-06,\n",
              "  2.446584403514862e-06,\n",
              "  2.5955960154533386e-06,\n",
              "  2.5904737412929535e-06,\n",
              "  2.57883220911026e-06,\n",
              "  2.5690533220767975e-06,\n",
              "  2.3278407752513885e-06,\n",
              "  2.3972243070602417e-06,\n",
              "  2.391636371612549e-06,\n",
              "  2.705492079257965e-06,\n",
              "  2.2901222109794617e-06,\n",
              "  2.543441951274872e-06,\n",
              "  2.671964466571808e-06,\n",
              "  2.4084001779556274e-06,\n",
              "  2.6123598217964172e-06,\n",
              "  2.4461187422275543e-06,\n",
              "  2.4507753551006317e-06,\n",
              "  2.4382025003433228e-06,\n",
              "  2.3120082914829254e-06,\n",
              "  2.302229404449463e-06,\n",
              "  2.339482307434082e-06,\n",
              "  2.4423934519290924e-06,\n",
              "  2.216547727584839e-06,\n",
              "  2.3120082914829254e-06,\n",
              "  2.184417098760605e-06,\n",
              "  2.3157335817813873e-06,\n",
              "  2.3618340492248535e-06,\n",
              "  2.503860741853714e-06,\n",
              "  2.4940818548202515e-06,\n",
              "  2.3497268557548523e-06,\n",
              "  2.45729461312294e-06,\n",
              "  2.137385308742523e-06,\n",
              "  2.455897629261017e-06,\n",
              "  2.478715032339096e-06,\n",
              "  2.2980384528636932e-06,\n",
              "  2.0829029381275177e-06,\n",
              "  2.352055162191391e-06,\n",
              "  2.416316419839859e-06,\n",
              "  2.0298175513744354e-06,\n",
              "  2.137385308742523e-06,\n",
              "  2.289190888404846e-06,\n",
              "  2.21841037273407e-06,\n",
              "  2.332031726837158e-06,\n",
              "  2.3031607270240784e-06,\n",
              "  2.196524292230606e-06,\n",
              "  2.1471641957759857e-06,\n",
              "  2.1248124539852142e-06,\n",
              "  2.3241154849529266e-06,\n",
              "  2.175569534301758e-06,\n",
              "  2.2337771952152252e-06,\n",
              "  2.223532646894455e-06,\n",
              "  2.1792948246002197e-06,\n",
              "  2.030283212661743e-06,\n",
              "  2.1439045667648315e-06,\n",
              "  2.3026950657367706e-06,\n",
              "  2.040993422269821e-06,\n",
              "  2.0060688257217407e-06,\n",
              "  2.062879502773285e-06,\n",
              "  1.9883736968040466e-06,\n",
              "  1.9604340195655823e-06,\n",
              "  2.05356627702713e-06,\n",
              "  2.021435648202896e-06,\n",
              "  1.9399449229240417e-06,\n",
              "  2.2025778889656067e-06,\n",
              "  2.1364539861679077e-06,\n",
              "  2.0372681319713593e-06,\n",
              "  1.9939616322517395e-06,\n",
              "  2.0209699869155884e-06,\n",
              "  1.968350261449814e-06,\n",
              "  1.9273720681667328e-06,\n",
              "  2.1103769540786743e-06,\n",
              "  1.923646777868271e-06,\n",
              "  2.3441389203071594e-06,\n",
              "  2.071727067232132e-06,\n",
              "  2.013053745031357e-06,\n",
              "  1.8579885363578796e-06,\n",
              "  2.1979212760925293e-06,\n",
              "  1.9674189388751984e-06,\n",
              "  2.0992010831832886e-06,\n",
              "  2.0745210349559784e-06,\n",
              "  1.9073486328125e-06,\n",
              "  1.9492581486701965e-06,\n",
              "  1.8891878426074982e-06,\n",
              "  1.8300488591194153e-06,\n",
              "  1.8524006009101868e-06,\n",
              "  2.0582228899002075e-06,\n",
              "  1.8673017621040344e-06,\n",
              "  1.8612481653690338e-06,\n",
              "  1.9264407455921173e-06,\n",
              "  1.9511207938194275e-06,\n",
              "  1.8114224076271057e-06,\n",
              "  1.864507794380188e-06,\n",
              "  1.948326826095581e-06,\n",
              "  1.8831342458724976e-06,\n",
              "  1.7713755369186401e-06,\n",
              "  1.8193386495113373e-06,\n",
              "  1.8998980522155762e-06,\n",
              "  1.8845312297344208e-06,\n",
              "  1.8356367945671082e-06,\n",
              "  1.910608261823654e-06,\n",
              "  1.918058842420578e-06,\n",
              "  1.8416903913021088e-06,\n",
              "  1.7192214727401733e-06,\n",
              "  1.7499551177024841e-06,\n",
              "  1.7704442143440247e-06,\n",
              "  1.9078142940998077e-06,\n",
              "  1.6856938600540161e-06,\n",
              "  1.8919818103313446e-06,\n",
              "  1.6777776181697845e-06,\n",
              "  1.7262063920497894e-06,\n",
              "  1.7760321497917175e-06,\n",
              "  1.8961727619171143e-06,\n",
              "  1.6745179891586304e-06,\n",
              "  1.7099082469940186e-06,\n",
              "  1.816079020500183e-06,\n",
              "  1.6898848116397858e-06,\n",
              "  1.7308630049228668e-06,\n",
              "  1.7564743757247925e-06,\n",
              "  1.6652047634124756e-06,\n",
              "  1.7886050045490265e-06,\n",
              "  1.684296876192093e-06,\n",
              "  1.6381964087486267e-06,\n",
              "  1.7327256500720978e-06,\n",
              "  1.7266720533370972e-06,\n",
              "  1.705717295408249e-06,\n",
              "  1.6740523278713226e-06,\n",
              "  1.6414560377597809e-06,\n",
              "  1.7331913113594055e-06,\n",
              "  1.7941929399967194e-06,\n",
              "  1.6693957149982452e-06,\n",
              "  1.6740523278713226e-06,\n",
              "  1.559033989906311e-06,\n",
              "  1.642853021621704e-06,\n",
              "  1.6461126506328583e-06,\n",
              "  1.6493722796440125e-06,\n",
              "  1.7634592950344086e-06,\n",
              "  1.6512349247932434e-06,\n",
              "  1.6130506992340088e-06,\n",
              "  1.684296876192093e-06,\n",
              "  1.7667189240455627e-06,\n",
              "  1.6205012798309326e-06,\n",
              "  1.7257407307624817e-06,\n",
              "  1.7890706658363342e-06,\n",
              "  1.6065314412117004e-06,\n",
              "  1.6437843441963196e-06,\n",
              "  1.614447683095932e-06,\n",
              "  1.618172973394394e-06,\n",
              "  1.6065314412117004e-06,\n",
              "  1.5972182154655457e-06,\n",
              "  1.7075799405574799e-06,\n",
              "  1.6205012798309326e-06,\n",
              "  1.627020537853241e-06,\n",
              "  1.7085112631320953e-06,\n",
              "  1.6712583601474762e-06,\n",
              "  1.6656704246997833e-06,\n",
              "  1.4896504580974579e-06,\n",
              "  1.7415732145309448e-06,\n",
              "  1.6293488442897797e-06,\n",
              "  1.5478581190109253e-06,\n",
              "  1.4691613614559174e-06,\n",
              "  1.5995465219020844e-06,\n",
              "  1.5469267964363098e-06,\n",
              "  1.5487894415855408e-06,\n",
              "  1.6046687960624695e-06,\n",
              "  1.6642734408378601e-06,\n",
              "  1.519918441772461e-06,\n",
              "  1.4565885066986084e-06,\n",
              "  1.516193151473999e-06,\n",
              "  1.530628651380539e-06,\n",
              "  1.5157274901866913e-06,\n",
              "  1.4873221516609192e-06,\n",
              "  1.51805579662323e-06,\n",
              "  1.5152618288993835e-06,\n",
              "  1.4211982488632202e-06,\n",
              "  1.6023404896259308e-06,\n",
              "  1.4980323612689972e-06,\n",
              "  1.5450641512870789e-06,\n",
              "  1.5660189092159271e-06,\n",
              "  1.3811513781547546e-06,\n",
              "  1.4905817806720734e-06,\n",
              "  1.5757977962493896e-06,\n",
              "  1.5064142644405365e-06,\n",
              "  1.526903361082077e-06,\n",
              "  1.4058314263820648e-06,\n",
              "  1.5944242477416992e-06,\n",
              "  1.409091055393219e-06,\n",
              "  1.421663910150528e-06,\n",
              "  1.369975507259369e-06,\n",
              "  1.505482941865921e-06,\n",
              "  1.4146789908409119e-06,\n",
              "  1.4598481357097626e-06,\n",
              "  1.4812685549259186e-06,\n",
              "  1.3811513781547546e-06,\n",
              "  1.3979151844978333e-06,\n",
              "  1.3899989426136017e-06,\n",
              "  1.287553459405899e-06,\n",
              "  1.3639219105243683e-06,\n",
              "  1.4998950064182281e-06,\n",
              "  1.4263205230236053e-06,\n",
              "  1.416075974702835e-06,\n",
              "  1.3811513781547546e-06,\n",
              "  1.4416873455047607e-06,\n",
              "  1.373235136270523e-06,\n",
              "  1.3746321201324463e-06,\n",
              "  1.4239922165870667e-06,\n",
              "  1.3834796845912933e-06,\n",
              "  1.3578683137893677e-06,\n",
              "  1.4416873455047607e-06,\n",
              "  1.432374119758606e-06,\n",
              "  1.3867393136024475e-06,\n",
              "  1.3671815395355225e-06,\n",
              "  1.4253892004489899e-06,\n",
              "  1.3266690075397491e-06,\n",
              "  1.437496393918991e-06,\n",
              "  1.4100223779678345e-06,\n",
              "  1.407228410243988e-06,\n",
              "  1.405365765094757e-06,\n",
              "  1.3569369912147522e-06,\n",
              "  1.344829797744751e-06,\n",
              "  1.402106136083603e-06,\n",
              "  1.3755634427070618e-06,\n",
              "  1.2754462659358978e-06,\n",
              "  1.4668330550193787e-06,\n",
              "  1.3238750398159027e-06,\n",
              "  1.3140961527824402e-06,\n",
              "  1.409091055393219e-06,\n",
              "  1.2973323464393616e-06,\n",
              "  1.528766006231308e-06,\n",
              "  1.2810342013835907e-06,\n",
              "  1.227017492055893e-06,\n",
              "  1.3057142496109009e-06,\n",
              "  1.3257376849651337e-06,\n",
              "  1.276843249797821e-06,\n",
              "  1.39651820063591e-06,\n",
              "  1.285690814256668e-06,\n",
              "  1.3979151844978333e-06,\n",
              "  1.2293457984924316e-06,\n",
              "  1.2652017176151276e-06,\n",
              "  1.2475065886974335e-06,\n",
              "  1.2950040400028229e-06,\n",
              "  1.3830140233039856e-06,\n",
              "  1.2461096048355103e-06,\n",
              "  1.3425014913082123e-06,\n",
              "  1.2954697012901306e-06,\n",
              "  1.255422830581665e-06,\n",
              "  1.2200325727462769e-06,\n",
              "  1.2544915080070496e-06,\n",
              "  1.3387762010097504e-06,\n",
              "  1.2004747986793518e-06,\n",
              "  1.264270395040512e-06,\n",
              "  1.3778917491436005e-06,\n",
              "  1.3005919754505157e-06,\n",
              "  1.2260861694812775e-06,\n",
              "  1.2908130884170532e-06,\n",
              "  1.1781230568885803e-06,\n",
              "  1.2977980077266693e-06,\n",
              "  1.3811513781547546e-06,\n",
              "  1.2335367500782013e-06,\n",
              "  1.223292201757431e-06,\n",
              "  1.173466444015503e-06,\n",
              "  1.2828968465328217e-06,\n",
              "  1.2302771210670471e-06,\n",
              "  1.2600794434547424e-06,\n",
              "  1.2633390724658966e-06,\n",
              "  1.2209638953208923e-06,\n",
              "  1.1981464922428131e-06,\n",
              "  1.1255033314228058e-06,\n",
              "  1.307111233472824e-06,\n",
              "  1.171603798866272e-06,\n",
              "  1.3280659914016724e-06,\n",
              "  1.2293457984924316e-06,\n",
              "  1.1953525245189667e-06,\n",
              "  1.2363307178020477e-06,\n",
              "  1.2088567018508911e-06,\n",
              "  1.3248063623905182e-06,\n",
              "  1.214444637298584e-06,\n",
              "  1.1655502021312714e-06,\n",
              "  1.1781230568885803e-06,\n",
              "  1.2530945241451263e-06,\n",
              "  1.225154846906662e-06,\n",
              "  1.171603798866272e-06,\n",
              "  1.2028031051158905e-06,\n",
              "  1.2093223631381989e-06,\n",
              "  1.1506490409374237e-06,\n",
              "  1.2316741049289703e-06,\n",
              "  1.178588718175888e-06,\n",
              "  1.1967495083808899e-06,\n",
              "  1.1976808309555054e-06,\n",
              "  1.2028031051158905e-06,\n",
              "  1.1562369763851166e-06,\n",
              "  1.0873191058635712e-06,\n",
              "  1.2479722499847412e-06,\n",
              "  1.2461096048355103e-06,\n",
              "  1.0998919606208801e-06,\n",
              "  1.2558884918689728e-06,\n",
              "  1.0728836059570312e-06,\n",
              "  1.1296942830085754e-06,\n",
              "  1.2121163308620453e-06,\n",
              "  1.1348165571689606e-06,\n",
              "  1.2028031051158905e-06,\n",
              "  1.1450611054897308e-06,\n",
              "  1.1567026376724243e-06,\n",
              "  1.1418014764785767e-06,\n",
              "  1.1720694601535797e-06,\n",
              "  1.1702068150043488e-06,\n",
              "  1.1078082025051117e-06,\n",
              "  1.0700896382331848e-06,\n",
              "  1.2111850082874298e-06,\n",
              "  1.1380761861801147e-06,\n",
              "  1.1427327990531921e-06,\n",
              "  1.0998919606208801e-06,\n",
              "  1.1711381375789642e-06,\n",
              "  1.1506490409374237e-06,\n",
              "  1.259148120880127e-06,\n",
              "  1.1362135410308838e-06,\n",
              "  1.1436641216278076e-06,\n",
              "  1.2102536857128143e-06,\n",
              "  1.1059455573558807e-06,\n",
              "  1.0547228157520294e-06,\n",
              "  1.1031515896320343e-06,\n",
              "  1.0831281542778015e-06,\n",
              "  1.0114163160324097e-06,\n",
              "  1.157168298959732e-06,\n",
              "  1.0970979928970337e-06,\n",
              "  1.1310912668704987e-06,\n",
              "  1.2489035725593567e-06,\n",
              "  1.1189840734004974e-06,\n",
              "  1.0514631867408752e-06,\n",
              "  1.0873191058635712e-06,\n",
              "  1.07521191239357e-06,\n",
              "  1.1106021702289581e-06,\n",
              "  1.07521191239357e-06,\n",
              "  1.126900315284729e-06,\n",
              "  1.148320734500885e-06,\n",
              "  1.062639057636261e-06,\n",
              "  1.1408701539039612e-06,\n",
              "  1.0691583156585693e-06,\n",
              "  1.0123476386070251e-06,\n",
              "  1.141335815191269e-06,\n",
              "  1.026783138513565e-06,\n",
              "  1.0617077350616455e-06,\n",
              "  1.055654138326645e-06,\n",
              "  1.014210283756256e-06,\n",
              "  1.0253861546516418e-06,\n",
              "  1.0444782674312592e-06,\n",
              "  1.1390075087547302e-06,\n",
              "  1.041218638420105e-06,\n",
              "  9.85804945230484e-07,\n",
              "  1.0030344128608704e-06,\n",
              "  1.0030344128608704e-06,\n",
              "  1.005362719297409e-06,\n",
              "  1.107342541217804e-06,\n",
              "  1.087784767150879e-06,\n",
              "  1.0300427675247192e-06,\n",
              "  1.0365620255470276e-06,\n",
              "  1.0547228157520294e-06,\n",
              "  1.0766088962554932e-06,\n",
              "  9.848736226558685e-07,\n",
              "  1.014210283756256e-06,\n",
              "  1.0100193321704865e-06,\n",
              "  1.0826624929904938e-06,\n",
              "  1.0239891707897186e-06,\n",
              "  9.51811671257019e-07,\n",
              "  1.0968319656967651e-06,\n",
              "  1.0719522833824158e-06,\n",
              "  1.0067597031593323e-06,\n",
              "  1.043081283569336e-06,\n",
              "  9.75094735622406e-07,\n",
              "  9.578652679920197e-07,\n",
              "  1.0775402188301086e-06,\n",
              "  1.032371073961258e-06,\n",
              "  9.73232090473175e-07,\n",
              "  9.881332516670227e-07,\n",
              "  9.76957380771637e-07,\n",
              "  9.676441550254822e-07,\n",
              "  9.746290743350983e-07,\n",
              "  1.0067597031593323e-06,\n",
              "  1.000240445137024e-06,\n",
              "  9.406358003616333e-07,\n",
              "  1.0249204933643341e-06,\n",
              "  9.797513484954834e-07,\n",
              "  9.587965905666351e-07,\n",
              "  1.0086223483085632e-06,\n",
              "  9.41101461648941e-07,\n",
              "  9.96515154838562e-07,\n",
              "  9.997747838497162e-07,\n",
              "  9.885989129543304e-07,\n",
              "  9.369105100631714e-07,\n",
              "  9.64384526014328e-07,\n",
              "  9.94652509689331e-07,\n",
              "  9.848736226558685e-07,\n",
              "  9.932555258274078e-07,\n",
              "  9.885989129543304e-07,\n",
              "  1.0044313967227936e-06,\n",
              "  9.499490261077881e-07,\n",
              "  1.005362719297409e-06,\n",
              "  1.0170042514801025e-06,\n",
              "  1.0007061064243317e-06,\n",
              "  8.810311555862427e-07,\n",
              "  9.513460099697113e-07,\n",
              "  9.178183972835541e-07,\n",
              "  9.71369445323944e-07,\n",
              "  9.685754776000977e-07,\n",
              "  9.42964106798172e-07,\n",
              "  9.015202522277832e-07,\n",
              "  9.73232090473175e-07,\n",
              "  9.57399606704712e-07,\n",
              "  9.192153811454773e-07,\n",
              "  9.136274456977844e-07,\n",
              "  9.173527359962463e-07,\n",
              "  9.10833477973938e-07,\n",
              "  8.824281394481659e-07,\n",
              "  9.5367431640625e-07,\n",
              "  8.754432201385498e-07,\n",
              "  9.010545909404755e-07,\n",
              "  9.19681042432785e-07,\n",
              "  9.480863809585571e-07,\n",
              "  9.480863809585571e-07,\n",
              "  9.033828973770142e-07,\n",
              "  9.285286068916321e-07,\n",
              "  9.313225746154785e-07,\n",
              "  9.043142199516296e-07,\n",
              "  8.712522685527802e-07,\n",
              "  9.057112038135529e-07,\n",
              "  8.745118975639343e-07,\n",
              "  9.112991392612457e-07,\n",
              "  9.187497198581696e-07,\n",
              "  8.903443813323975e-07,\n",
              "  9.182840585708618e-07,\n",
              "  8.936040103435516e-07,\n",
              "  9.289942681789398e-07,\n",
              "  9.103678166866302e-07,\n",
              "  9.122304618358612e-07,\n",
              "  9.038485586643219e-07,\n",
              "  9.345822036266327e-07,\n",
              "  9.550713002681732e-07,\n",
              "  8.596107363700867e-07,\n",
              "  9.154900908470154e-07,\n",
              "  1.0067597031593323e-06,\n",
              "  8.563511073589325e-07,\n",
              "  9.331852197647095e-07,\n",
              "  8.945353329181671e-07,\n",
              "  8.549541234970093e-07,\n",
              "  9.275972843170166e-07,\n",
              "  9.262003004550934e-07,\n",
              "  8.977949619293213e-07,\n",
              "  8.423812687397003e-07,\n",
              "  8.475035429000854e-07,\n",
              "  8.884817361831665e-07,\n",
              "  9.248033165931702e-07,\n",
              "  8.4657222032547e-07,\n",
              "  8.651986718177795e-07,\n",
              "  8.326023817062378e-07,\n",
              "  8.596107363700867e-07,\n",
              "  8.735805749893188e-07,\n",
              "  8.651986718177795e-07,\n",
              "  8.889473974704742e-07,\n",
              "  8.395873010158539e-07,\n",
              "  8.586794137954712e-07,\n",
              "  8.479692041873932e-07,\n",
              "  8.693896234035492e-07,\n",
              "  7.837079465389252e-07,\n",
              "  8.959323167800903e-07,\n",
              "  8.684583008289337e-07,\n",
              "  8.596107363700867e-07,\n",
              "  8.190982043743134e-07,\n",
              "  8.987262845039368e-07,\n",
              "  8.936040103435516e-07,\n",
              "  8.33999365568161e-07,\n",
              "  8.489005267620087e-07,\n",
              "  8.76840204000473e-07,\n",
              "  8.684583008289337e-07,\n",
              "  8.270144462585449e-07,\n",
              "  8.409842848777771e-07,\n",
              "  8.591450750827789e-07,\n",
              "  7.795169949531555e-07,\n",
              "  9.150244295597076e-07,\n",
              "  8.828938007354736e-07,\n",
              "  8.149072527885437e-07,\n",
              "  8.004717528820038e-07,\n",
              "  8.57282429933548e-07,\n",
              "  8.223578333854675e-07,\n",
              "  7.953494787216187e-07,\n",
              "  8.656643331050873e-07,\n",
              "  8.349306881427765e-07,\n",
              "  8.507631719112396e-07,\n",
              "  7.958151400089264e-07,\n",
              "  7.851049304008484e-07,\n",
              "  7.869675755500793e-07,\n",
              "  7.925555109977722e-07,\n",
              "  8.14441591501236e-07,\n",
              "  8.260831236839294e-07,\n",
              "  8.586794137954712e-07,\n",
              "  8.456408977508545e-07,\n",
              "  8.10716301202774e-07,\n",
              "  9.019859135150909e-07,\n",
              "  7.622875273227692e-07,\n",
              "  8.041970431804657e-07,\n",
              "  8.707866072654724e-07,\n",
              "  8.996576070785522e-07,\n",
              "  7.902272045612335e-07,\n",
              "  8.135102689266205e-07,\n",
              "  7.478520274162292e-07,\n",
              "  7.860362529754639e-07,\n",
              "  8.321367204189301e-07,\n",
              "  8.041970431804657e-07,\n",
              "  7.515773177146912e-07,\n",
              "  7.916241884231567e-07,\n",
              "  8.139759302139282e-07,\n",
              "  8.065253496170044e-07,\n",
              "  8.307397365570068e-07,\n",
              "  7.580965757369995e-07,\n",
              "  8.274801075458527e-07,\n",
              "  8.349306881427765e-07,\n",
              "  8.591450750827789e-07,\n",
              "  7.874332368373871e-07,\n",
              "  8.475035429000854e-07,\n",
              "  7.934868335723877e-07,\n",
              "  7.799826562404633e-07,\n",
              "  7.874332368373871e-07,\n",
              "  8.335337042808533e-07,\n",
              "  8.242204785346985e-07,\n",
              "  7.89295881986618e-07,\n",
              "  7.976777851581573e-07,\n",
              "  7.427297532558441e-07,\n",
              "  7.888302206993103e-07,\n",
              "  8.381903171539307e-07,\n",
              "  7.818453013896942e-07,\n",
              "  7.189810276031494e-07,\n",
              "  8.041970431804657e-07,\n",
              "  7.813796401023865e-07,\n",
              "  7.441267371177673e-07,\n",
              "  8.828938007354736e-07,\n",
              "  8.284114301204681e-07,\n",
              "  8.03731381893158e-07,\n",
              "  7.399357855319977e-07,\n",
              "  7.371418178081512e-07,\n",
              "  8.209608495235443e-07,\n",
              "  8.097849786281586e-07,\n",
              "  7.562339305877686e-07,\n",
              "  8.097849786281586e-07,\n",
              "  7.413327693939209e-07,\n",
              "  8.00006091594696e-07,\n",
              "  7.660128176212311e-07,\n",
              "  7.59027898311615e-07,\n",
              "  7.301568984985352e-07,\n",
              "  7.692724466323853e-07,\n",
              "  7.348135113716125e-07,\n",
              "  8.190982043743134e-07,\n",
              "  7.60890543460846e-07,\n",
              "  7.46455043554306e-07,\n",
              "  7.702037692070007e-07,\n",
              "  7.352791726589203e-07,\n",
              "  7.497146725654602e-07,\n",
              "  7.57165253162384e-07,\n",
              "  7.515773177146912e-07,\n",
              "  7.231719791889191e-07,\n",
              "  7.585622370243073e-07,\n",
              "  7.497146725654602e-07,\n",
              "  7.371418178081512e-07,\n",
              "  7.306225597858429e-07,\n",
              "  7.89295881986618e-07,\n",
              "  7.390044629573822e-07,\n",
              "  7.436610758304596e-07,\n",
              "  7.441267371177673e-07,\n",
              "  7.46455043554306e-07,\n",
              "  7.557682693004608e-07,\n",
              "  7.338821887969971e-07,\n",
              "  7.492490112781525e-07,\n",
              "  7.241033017635345e-07,\n",
              "  7.255002856254578e-07,\n",
              "  6.691552698612213e-07,\n",
              "  7.05476850271225e-07,\n",
              "  6.910413503646851e-07,\n",
              "  6.919726729393005e-07,\n",
              "  7.189810276031494e-07,\n",
              "  7.096678018569946e-07,\n",
              "  7.431954145431519e-07,\n",
              "  7.352791726589203e-07,\n",
              "  7.580965757369995e-07,\n",
              "  7.012858986854553e-07,\n",
              "  7.115304470062256e-07,\n",
              "  7.199123501777649e-07,\n",
              "  7.026828825473785e-07,\n",
              "  7.329508662223816e-07,\n",
              "  6.994232535362244e-07,\n",
              "  6.677582859992981e-07,\n",
              "  7.860362529754639e-07,\n",
              "  6.919726729393005e-07,\n",
              "  6.491318345069885e-07,\n",
              "  8.14441591501236e-07,\n",
              "  7.026828825473785e-07,\n",
              "  7.7858567237854e-07,\n",
              "  7.124617695808411e-07,\n",
              "  6.663613021373749e-07,\n",
              "  6.807968020439148e-07,\n",
              "  6.845220923423767e-07,\n",
              "  7.16187059879303e-07,\n",
              "  6.919726729393005e-07,\n",
              "  7.110647857189178e-07,\n",
              "  7.622875273227692e-07,\n",
              "  6.654299795627594e-07,\n",
              "  6.547197699546814e-07,\n",
              "  6.82193785905838e-07,\n",
              "  7.031485438346863e-07,\n",
              "  7.217749953269958e-07,\n",
              "  6.84056431055069e-07,\n",
              "  7.296912372112274e-07,\n",
              "  6.416812539100647e-07,\n",
              "  6.644986569881439e-07,\n",
              "  7.809139788150787e-07,\n",
              "  7.585622370243073e-07,\n",
              "  6.644986569881439e-07,\n",
              "  7.580965757369995e-07,\n",
              "  7.101334631443024e-07,\n",
              "  6.379559636116028e-07,\n",
              "  6.28642737865448e-07,\n",
              "  6.882473826408386e-07,\n",
              "  6.402842700481415e-07,\n",
              "  6.612390279769897e-07,\n",
              "  7.413327693939209e-07,\n",
              "  6.617046892642975e-07,\n",
              "  6.477348506450653e-07,\n",
              "  6.635673344135284e-07,\n",
              "  6.677582859992981e-07,\n",
              "  6.747432053089142e-07,\n",
              "  6.868503987789154e-07,\n",
              "  6.309710443019867e-07,\n",
              "  6.849877536296844e-07,\n",
              "  6.337650120258331e-07,\n",
              "  7.278285920619965e-07,\n",
              "  7.147900760173798e-07,\n",
              "  6.868503987789154e-07,\n",
              "  7.05476850271225e-07,\n",
              "  6.556510925292969e-07,\n",
              "  7.059425115585327e-07,\n",
              "  6.463378667831421e-07,\n",
              "  6.766058504581451e-07,\n",
              "  6.654299795627594e-07,\n",
              "  6.449408829212189e-07,\n",
              "  7.227063179016113e-07,\n",
              "  6.789341568946838e-07,\n",
              "  7.329508662223816e-07,\n",
              "  6.933696568012238e-07,\n",
              "  6.919726729393005e-07,\n",
              "  6.537884473800659e-07,\n",
              "  6.682239472866058e-07,\n",
              "  7.133930921554565e-07,\n",
              "  6.807968020439148e-07,\n",
              "  6.789341568946838e-07,\n",
              "  6.766058504581451e-07,\n",
              "  6.635673344135284e-07,\n",
              "  6.486661732196808e-07,\n",
              "  6.235204637050629e-07,\n",
              "  6.826594471931458e-07,\n",
              "  6.831251084804535e-07,\n",
              "  6.388872861862183e-07,\n",
              "  6.332993507385254e-07,\n",
              "  6.649643182754517e-07,\n",
              "  6.598420441150665e-07,\n",
              "  6.528571248054504e-07,\n",
              "  6.118789315223694e-07,\n",
              "  6.468035280704498e-07,\n",
              "  6.724148988723755e-07,\n",
              "  6.491318345069885e-07,\n",
              "  6.398186087608337e-07,\n",
              "  6.239861249923706e-07,\n",
              "  6.30505383014679e-07,\n",
              "  6.123445928096771e-07,\n",
              "  5.94649463891983e-07,\n",
              "  6.044283509254456e-07,\n",
              "  6.379559636116028e-07,\n",
              "  6.603077054023743e-07,\n",
              "  6.94766640663147e-07,\n",
              "  5.979090929031372e-07,\n",
              "  6.644986569881439e-07,\n",
              "  6.258487701416016e-07,\n",
              "  6.118789315223694e-07,\n",
              "  6.672926247119904e-07,\n",
              "  6.747432053089142e-07,\n",
              "  6.584450602531433e-07,\n",
              "  6.281770765781403e-07,\n",
              "  6.021000444889069e-07,\n",
              "  6.067566573619843e-07,\n",
              "  6.104819476604462e-07,\n",
              "  6.07222318649292e-07,\n",
              "  6.277114152908325e-07,\n",
              "  6.775371730327606e-07,\n",
              "  5.974434316158295e-07,\n",
              "  7.05476850271225e-07,\n",
              "  5.792826414108276e-07,\n",
              "  6.076879799365997e-07,\n",
              "  5.606561899185181e-07,\n",
              "  5.890615284442902e-07,\n",
              "  6.211921572685242e-07,\n",
              "  5.797483026981354e-07,\n",
              "  6.402842700481415e-07,\n",
              "  6.225891411304474e-07,\n",
              "  6.034970283508301e-07,\n",
              "  5.746260285377502e-07,\n",
              "  5.96512109041214e-07,\n",
              "  6.174668669700623e-07,\n",
              "  6.826594471931458e-07,\n",
              "  6.631016731262207e-07,\n",
              "  6.104819476604462e-07,\n",
              "  6.170012056827545e-07,\n",
              "  5.853362381458282e-07,\n",
              "  6.346963346004486e-07,\n",
              "  6.058253347873688e-07,\n",
              "  6.216578185558319e-07,\n",
              "  5.885958671569824e-07,\n",
              "  6.39352947473526e-07,\n",
              "  5.876645445823669e-07,\n",
              "  6.025657057762146e-07,\n",
              "  5.825422704219818e-07,\n",
              "  5.62518835067749e-07,\n",
              "  5.844049155712128e-07,\n",
              "  6.118789315223694e-07,\n",
              "  6.370246410369873e-07,\n",
              "  5.969777703285217e-07,\n",
              "  5.778856575489044e-07,\n",
              "  6.034970283508301e-07,\n",
              "  6.09084963798523e-07,\n",
              "  5.871988832950592e-07,\n",
              "  6.044283509254456e-07,\n",
              "  6.118789315223694e-07,\n",
              "  5.853362381458282e-07,\n",
              "  6.132759153842926e-07,\n",
              "  5.816109478473663e-07,\n",
              "  5.611218512058258e-07,\n",
              "  5.653128027915955e-07,\n",
              "  5.648471415042877e-07,\n",
              "  5.974434316158295e-07,\n",
              "  5.527399480342865e-07,\n",
              "  5.760230123996735e-07,\n",
              "  5.797483026981354e-07,\n",
              "  5.601905286312103e-07,\n",
              "  5.699694156646729e-07,\n",
              "  5.690380930900574e-07,\n",
              "  6.07222318649292e-07,\n",
              "  5.541369318962097e-07,\n",
              "  5.774199962615967e-07,\n",
              "  5.885958671569824e-07,\n",
              "  5.909241735935211e-07,\n",
              "  5.969777703285217e-07,\n",
              "  5.94649463891983e-07,\n",
              "  5.774199962615967e-07,\n",
              "  5.434267222881317e-07,\n",
              "  5.881302058696747e-07,\n",
              "  6.132759153842926e-07,\n",
              "  5.578622221946716e-07,\n",
              "  5.909241735935211e-07,\n",
              "  5.424953997135162e-07,\n",
              "  5.704350769519806e-07,\n",
              "  5.261972546577454e-07,\n",
              "  5.364418029785156e-07,\n",
              "  6.170012056827545e-07,\n",
              "  5.597248673439026e-07,\n",
              "  5.615875124931335e-07,\n",
              "  5.480833351612091e-07,\n",
              "  5.327165126800537e-07,\n",
              "  5.499459803104401e-07,\n",
              "  5.587935447692871e-07,\n",
              "  6.07222318649292e-07,\n",
              "  6.263144314289093e-07,\n",
              "  5.615875124931335e-07,\n",
              "  5.438923835754395e-07,\n",
              "  5.755573511123657e-07,\n",
              "  5.415640771389008e-07,\n",
              "  5.336478352546692e-07,\n",
              "  5.406327545642853e-07,\n",
              "  5.438923835754395e-07,\n",
              "  6.002373993396759e-07,\n",
              "  5.448237061500549e-07,\n",
              "  5.774199962615967e-07,\n",
              "  5.578622221946716e-07,\n",
              "  5.285255610942841e-07,\n",
              "  5.937181413173676e-07,\n",
              "  5.73229044675827e-07,\n",
              "  5.718320608139038e-07,\n",
              "  6.183981895446777e-07,\n",
              "  5.08967787027359e-07,\n",
              "  5.378387868404388e-07,\n",
              "  5.51808625459671e-07,\n",
              "  5.289912223815918e-07,\n",
              "  5.620531737804413e-07,\n",
              "  6.100162863731384e-07,\n",
              "  5.336478352546692e-07,\n",
              "  5.224719643592834e-07,\n",
              "  5.452893674373627e-07,\n",
              "  5.555339157581329e-07,\n",
              "  5.490146577358246e-07,\n",
              "  5.494803190231323e-07,\n",
              "  5.480833351612091e-07,\n",
              "  5.07105141878128e-07,\n",
              "  5.289912223815918e-07,\n",
              "  5.457550287246704e-07,\n",
              "  5.378387868404388e-07,\n",
              "  5.485489964485168e-07,\n",
              "  5.527399480342865e-07,\n",
              "  5.341134965419769e-07,\n",
              "  5.438923835754395e-07,\n",
              "  5.317851901054382e-07,\n",
              "  5.373731255531311e-07,\n",
              "  5.746260285377502e-07,\n",
              "  5.434267222881317e-07,\n",
              "  5.979090929031372e-07,\n",
              "  5.359761416912079e-07,\n",
              "  5.126930773258209e-07,\n",
              "  5.19677996635437e-07,\n",
              "  5.294568836688995e-07,\n",
              "  5.657784640789032e-07,\n",
              "  5.434267222881317e-07,\n",
              "  5.490146577358246e-07,\n",
              "  4.85684722661972e-07,\n",
              "  4.973262548446655e-07,\n",
              "  5.210749804973602e-07,\n",
              "  ...],\n",
              " [1.0230578482151031e-06,\n",
              "  1.0342337191104889e-06,\n",
              "  1.0128132998943329e-06,\n",
              "  1.0901130735874176e-06,\n",
              "  1.0044313967227936e-06,\n",
              "  9.736977517604828e-07,\n",
              "  1.030508428812027e-06,\n",
              "  1.0039657354354858e-06,\n",
              "  9.932555258274078e-07,\n",
              "  9.75094735622406e-07,\n",
              "  9.797513484954834e-07,\n",
              "  1.0076910257339478e-06,\n",
              "  1.051928848028183e-06,\n",
              "  9.676441550254822e-07,\n",
              "  1.0100193321704865e-06,\n",
              "  1.0887160897254944e-06,\n",
              "  1.0598450899124146e-06,\n",
              "  1.062639057636261e-06,\n",
              "  1.0640360414981842e-06,\n",
              "  1.0682269930839539e-06,\n",
              "  1.0388903319835663e-06,\n",
              "  1.0114163160324097e-06,\n",
              "  1.0281801223754883e-06,\n",
              "  1.1022202670574188e-06,\n",
              "  9.378418326377869e-07,\n",
              "  9.862706065177917e-07,\n",
              "  1.096632331609726e-06,\n",
              "  1.030508428812027e-06,\n",
              "  1.0975636541843414e-06,\n",
              "  1.1119991540908813e-06,\n",
              "  1.1166557669639587e-06,\n",
              "  1.0277144610881805e-06,\n",
              "  1.1147931218147278e-06,\n",
              "  9.783543646335602e-07,\n",
              "  1.0100193321704865e-06,\n",
              "  9.853392839431763e-07,\n",
              "  9.667128324508667e-07,\n",
              "  1.0058283805847168e-06,\n",
              "  1.0095536708831787e-06,\n",
              "  1.0114163160324097e-06,\n",
              "  9.806826710700989e-07,\n",
              "  1.0686926543712616e-06,\n",
              "  9.438954293727875e-07,\n",
              "  1.0384246706962585e-06,\n",
              "  9.378418326377869e-07,\n",
              "  1.0123476386070251e-06,\n",
              "  1.0184012353420258e-06,\n",
              "  9.96515154838562e-07,\n",
              "  9.685754776000977e-07,\n",
              "  1.0058283805847168e-06,\n",
              "  1.084059476852417e-06,\n",
              "  1.0104849934577942e-06,\n",
              "  9.881332516670227e-07,\n",
              "  1.0561197996139526e-06,\n",
              "  9.704381227493286e-07,\n",
              "  1.0016374289989471e-06,\n",
              "  1.0076910257339478e-06,\n",
              "  1.1497177183628082e-06,\n",
              "  1.0128132998943329e-06,\n",
              "  1.0123476386070251e-06,\n",
              "  1.0230578482151031e-06,\n",
              "  1.0062940418720245e-06,\n",
              "  1.080334186553955e-06,\n",
              "  9.848736226558685e-07,\n",
              "  9.988434612751007e-07,\n",
              "  1.082196831703186e-06,\n",
              "  9.676441550254822e-07,\n",
              "  9.974464774131775e-07,\n",
              "  1.0123476386070251e-06,\n",
              "  1.0691583156585693e-06,\n",
              "  1.0277144610881805e-06,\n",
              "  1.1110678315162659e-06,\n",
              "  1.0086223483085632e-06,\n",
              "  9.783543646335602e-07,\n",
              "  1.0156072676181793e-06,\n",
              "  1.103617250919342e-06,\n",
              "  1.0165385901927948e-06,\n",
              "  1.0067597031593323e-06,\n",
              "  1.1003576219081879e-06,\n",
              "  1.0570511221885681e-06,\n",
              "  1.0365620255470276e-06,\n",
              "  1.0086223483085632e-06,\n",
              "  9.98377799987793e-07,\n",
              "  1.1739321053028107e-06,\n",
              "  1.1241063475608826e-06,\n",
              "  1.0989606380462646e-06,\n",
              "  1.0756775736808777e-06,\n",
              "  9.941868484020233e-07,\n",
              "  1.0309740900993347e-06,\n",
              "  1.1008232831954956e-06,\n",
              "  9.592622518539429e-07,\n",
              "  9.313225746154785e-07,\n",
              "  9.657815098762512e-07,\n",
              "  9.94652509689331e-07,\n",
              "  1.0016374289989471e-06,\n",
              "  1.0365620255470276e-06,\n",
              "  1.1580996215343475e-06,\n",
              "  1.0575167834758759e-06,\n",
              "  9.736977517604828e-07,\n",
              "  1.0891817510128021e-06,\n",
              "  1.0123476386070251e-06,\n",
              "  9.98377799987793e-07,\n",
              "  1.0421499609947205e-06,\n",
              "  1.058913767337799e-06,\n",
              "  1.0458752512931824e-06,\n",
              "  1.0454095900058746e-06,\n",
              "  1.0044313967227936e-06,\n",
              "  1.0095536708831787e-06,\n",
              "  1.0258518159389496e-06,\n",
              "  1.0221265256404877e-06,\n",
              "  1.0747462511062622e-06,\n",
              "  1.135747879743576e-06,\n",
              "  1.0887160897254944e-06,\n",
              "  9.862706065177917e-07,\n",
              "  1.0416842997074127e-06,\n",
              "  1.0612420737743378e-06,\n",
              "  1.0598450899124146e-06,\n",
              "  1.0542571544647217e-06,\n",
              "  1.1175870895385742e-06,\n",
              "  1.0435469448566437e-06,\n",
              "  1.0621733963489532e-06,\n",
              "  9.830109775066376e-07,\n",
              "  9.853392839431763e-07,\n",
              "  9.848736226558685e-07,\n",
              "  1.0905787348747253e-06,\n",
              "  9.96515154838562e-07,\n",
              "  1.092907041311264e-06,\n",
              "  1.0514631867408752e-06,\n",
              "  9.997747838497162e-07,\n",
              "  1.0640360414981842e-06,\n",
              "  1.0319054126739502e-06,\n",
              "  1.017935574054718e-06,\n",
              "  1.0612420737743378e-06,\n",
              "  1.1068768799304962e-06,\n",
              "  1.0444782674312592e-06,\n",
              "  1.0961666703224182e-06,\n",
              "  1.0128132998943329e-06,\n",
              "  1.0463409125804901e-06,\n",
              "  1.0854564607143402e-06,\n",
              "  1.103617250919342e-06,\n",
              "  1.0603107511997223e-06,\n",
              "  1.191161572933197e-06,\n",
              "  9.937211871147156e-07,\n",
              "  1.0719522833824158e-06,\n",
              "  1.0244548320770264e-06,\n",
              "  1.0272487998008728e-06,\n",
              "  1.005362719297409e-06,\n",
              "  1.0468065738677979e-06,\n",
              "  1.130625605583191e-06,\n",
              "  1.06077641248703e-06,\n",
              "  9.34116542339325e-07,\n",
              "  1.0319054126739502e-06,\n",
              "  1.1003576219081879e-06,\n",
              "  1.0300427675247192e-06,\n",
              "  1.0184012353420258e-06,\n",
              "  1.0472722351551056e-06,\n",
              "  1.00722536444664e-06,\n",
              "  1.0435469448566437e-06,\n",
              "  1.0812655091285706e-06,\n",
              "  9.844079613685608e-07,\n",
              "  9.811483323574066e-07,\n",
              "  1.0211952030658722e-06,\n",
              "  1.0454095900058746e-06,\n",
              "  1.016072928905487e-06,\n",
              "  1.028645783662796e-06,\n",
              "  1.0509975254535675e-06,\n",
              "  1.0919757187366486e-06,\n",
              "  9.592622518539429e-07,\n",
              "  1.0081566870212555e-06,\n",
              "  1.0542571544647217e-06,\n",
              "  1.107342541217804e-06,\n",
              "  1.0756775736808777e-06,\n",
              "  1.053791493177414e-06,\n",
              "  1.0686926543712616e-06,\n",
              "  1.0593794286251068e-06,\n",
              "  1.0933727025985718e-06,\n",
              "  1.0747462511062622e-06,\n",
              "  1.0775402188301086e-06,\n",
              "  9.927898645401e-07,\n",
              "  1.0477378964424133e-06,\n",
              "  1.0598450899124146e-06,\n",
              "  9.890645742416382e-07,\n",
              "  1.0170042514801025e-06,\n",
              "  9.64384526014328e-07,\n",
              "  1.1008232831954956e-06,\n",
              "  1.0263174772262573e-06,\n",
              "  1.103617250919342e-06,\n",
              "  9.89530235528946e-07,\n",
              "  1.043081283569336e-06,\n",
              "  1.0617077350616455e-06,\n",
              "  1.014210283756256e-06,\n",
              "  1.0421499609947205e-06,\n",
              "  1.0030344128608704e-06,\n",
              "  9.634532034397125e-07,\n",
              "  1.0463409125804901e-06,\n",
              "  9.78820025920868e-07,\n",
              "  1.0328367352485657e-06,\n",
              "  1.0225921869277954e-06,\n",
              "  1.1515803635120392e-06,\n",
              "  1.0309740900993347e-06,\n",
              "  1.0658986866474152e-06,\n",
              "  1.0444782674312592e-06,\n",
              "  1.0468065738677979e-06,\n",
              "  1.051928848028183e-06,\n",
              "  9.96515154838562e-07,\n",
              "  1.0202638804912567e-06,\n",
              "  1.048203557729721e-06,\n",
              "  1.0058283805847168e-06,\n",
              "  1.0514631867408752e-06,\n",
              "  1.0705552995204926e-06,\n",
              "  1.0775402188301086e-06,\n",
              "  1.0328367352485657e-06,\n",
              "  1.0621733963489532e-06,\n",
              "  9.727664291858673e-07,\n",
              "  1.043081283569336e-06,\n",
              "  9.718351066112518e-07,\n",
              "  1.009088009595871e-06,\n",
              "  1.0514631867408752e-06,\n",
              "  1.0319054126739502e-06,\n",
              "  1.1301599442958832e-06,\n",
              "  9.802170097827911e-07,\n",
              "  1.0649673640727997e-06,\n",
              "  1.0766088962554932e-06,\n",
              "  9.280629456043243e-07,\n",
              "  1.0132789611816406e-06,\n",
              "  1.0309740900993347e-06,\n",
              "  1.109205186367035e-06,\n",
              "  1.0533258318901062e-06,\n",
              "  9.764917194843292e-07,\n",
              "  1.041218638420105e-06,\n",
              "  1.0682269930839539e-06,\n",
              "  1.0756775736808777e-06,\n",
              "  1.0635703802108765e-06,\n",
              "  9.718351066112518e-07,\n",
              "  1.0509975254535675e-06,\n",
              "  1.0444782674312592e-06,\n",
              "  1.0505318641662598e-06,\n",
              "  1.0491348803043365e-06,\n",
              "  1.0244548320770264e-06,\n",
              "  1.0188668966293335e-06,\n",
              "  1.1059455573558807e-06,\n",
              "  9.825453162193298e-07,\n",
              "  1.0640360414981842e-06,\n",
              "  1.032371073961258e-06,\n",
              "  1.0156072676181793e-06,\n",
              "  1.00722536444664e-06,\n",
              "  1.000240445137024e-06,\n",
              "  9.867362678050995e-07,\n",
              "  1.0798685252666473e-06,\n",
              "  1.0300427675247192e-06,\n",
              "  1.0272487998008728e-06,\n",
              "  1.0360963642597198e-06,\n",
              "  1.0454095900058746e-06,\n",
              "  1.0440126061439514e-06,\n",
              "  1.0244548320770264e-06,\n",
              "  1.0612420737743378e-06,\n",
              "  1.0021030902862549e-06,\n",
              "  1.0067597031593323e-06,\n",
              "  1.0542571544647217e-06,\n",
              "  1.1185184121131897e-06,\n",
              "  1.0165385901927948e-06,\n",
              "  1.0882504284381866e-06,\n",
              "  1.1022202670574188e-06,\n",
              "  1.000240445137024e-06,\n",
              "  1.0468065738677979e-06,\n",
              "  1.0440126061439514e-06,\n",
              "  1.116190105676651e-06,\n",
              "  1.0011717677116394e-06,\n",
              "  1.0565854609012604e-06,\n",
              "  1.1101365089416504e-06,\n",
              "  1.0281801223754883e-06,\n",
              "  1.0472722351551056e-06,\n",
              "  1.0062940418720245e-06,\n",
              "  1.0272487998008728e-06,\n",
              "  1.0989606380462646e-06,\n",
              "  1.0565854609012604e-06,\n",
              "  1.0891817510128021e-06,\n",
              "  1.0123476386070251e-06,\n",
              "  1.0081566870212555e-06,\n",
              "  1.0342337191104889e-06,\n",
              "  1.0575167834758759e-06,\n",
              "  1.071486622095108e-06,\n",
              "  9.746290743350983e-07,\n",
              "  1.0211952030658722e-06,\n",
              "  1.0277144610881805e-06,\n",
              "  1.0575167834758759e-06,\n",
              "  1.0263174772262573e-06,\n",
              "  1.082196831703186e-06,\n",
              "  9.727664291858673e-07,\n",
              "  1.0365620255470276e-06,\n",
              "  1.00722536444664e-06,\n",
              "  1.0253861546516418e-06,\n",
              "  1.0328367352485657e-06,\n",
              "  1.0514631867408752e-06,\n",
              "  9.853392839431763e-07,\n",
              "  1.150183379650116e-06,\n",
              "  9.792856872081757e-07,\n",
              "  1.1273659765720367e-06,\n",
              "  9.913928806781769e-07,\n",
              "  1.1040829122066498e-06,\n",
              "  1.0016374289989471e-06,\n",
              "  1.1939555406570435e-06,\n",
              "  1.0370276868343353e-06,\n",
              "  1.0295771062374115e-06,\n",
              "  9.764917194843292e-07,\n",
              "  9.78820025920868e-07,\n",
              "  9.578652679920197e-07,\n",
              "  1.062639057636261e-06,\n",
              "  1.0230578482151031e-06,\n",
              "  1.0081566870212555e-06,\n",
              "  9.988434612751007e-07,\n",
              "  1.0700896382331848e-06,\n",
              "  1.1101365089416504e-06,\n",
              "  1.0505318641662598e-06,\n",
              "  1.0193325579166412e-06,\n",
              "  1.0402873158454895e-06,\n",
              "  9.87667590379715e-07,\n",
              "  9.560026228427887e-07,\n",
              "  1.048203557729721e-06,\n",
              "  1.0277144610881805e-06,\n",
              "  9.96515154838562e-07,\n",
              "  9.355135262012482e-07,\n",
              "  9.993091225624084e-07,\n",
              "  9.979121387004852e-07,\n",
              "  9.862706065177917e-07,\n",
              "  9.569339454174042e-07,\n",
              "  9.993091225624084e-07,\n",
              "  1.00722536444664e-06,\n",
              "  1.0388903319835663e-06,\n",
              "  1.058913767337799e-06,\n",
              "  1.0235235095024109e-06,\n",
              "  1.0165385901927948e-06,\n",
              "  1.017935574054718e-06,\n",
              "  1.0067597031593323e-06,\n",
              "  1.1809170246124268e-06,\n",
              "  9.848736226558685e-07,\n",
              "  1.0854564607143402e-06,\n",
              "  9.783543646335602e-07,\n",
              "  1.0724179446697235e-06,\n",
              "  1.2619420886039734e-06,\n",
              "  9.988434612751007e-07,\n",
              "  1.0137446224689484e-06,\n",
              "  1.0239891707897186e-06,\n",
              "  1.037493348121643e-06,\n",
              "  1.0612420737743378e-06,\n",
              "  1.0365620255470276e-06,\n",
              "  1.0100193321704865e-06,\n",
              "  1.0523945093154907e-06,\n",
              "  1.064501702785492e-06,\n",
              "  1.1129304766654968e-06,\n",
              "  9.718351066112518e-07,\n",
              "  1.0207295417785645e-06,\n",
              "  1.0919757187366486e-06,\n",
              "  1.06077641248703e-06,\n",
              "  1.0244548320770264e-06,\n",
              "  1.1003576219081879e-06,\n",
              "  1.1203810572624207e-06,\n",
              "  9.75094735622406e-07,\n",
              "  1.0812655091285706e-06,\n",
              "  1.0621733963489532e-06,\n",
              "  9.913928806781769e-07,\n",
              "  1.0454095900058746e-06,\n",
              "  1.0146759450435638e-06,\n",
              "  1.0668300092220306e-06,\n",
              "  1.003500074148178e-06,\n",
              "  1.0784715414047241e-06,\n",
              "  1.0356307029724121e-06,\n",
              "  9.746290743350983e-07,\n",
              "  1.0849907994270325e-06,\n",
              "  9.695068001747131e-07,\n",
              "  1.043081283569336e-06,\n",
              "  9.704381227493286e-07,\n",
              "  9.480863809585571e-07,\n",
              "  1.0044313967227936e-06,\n",
              "  1.0742805898189545e-06,\n",
              "  1.0388903319835663e-06,\n",
              "  1.0649673640727997e-06,\n",
              "  1.0542571544647217e-06,\n",
              "  9.620562195777893e-07,\n",
              "  1.1133961379528046e-06,\n",
              "  1.0058283805847168e-06,\n",
              "  1.0365620255470276e-06,\n",
              "  9.73232090473175e-07,\n",
              "  1.0346993803977966e-06,\n",
              "  9.913928806781769e-07,\n",
              "  9.746290743350983e-07,\n",
              "  1.0957010090351105e-06,\n",
              "  1.0039657354354858e-06,\n",
              "  1.0882504284381866e-06,\n",
              "  9.830109775066376e-07,\n",
              "  9.923242032527924e-07,\n",
              "  1.0170042514801025e-06,\n",
              "  1.0705552995204926e-06,\n",
              "  1.0547228157520294e-06,\n",
              "  1.0505318641662598e-06,\n",
              "  9.909272193908691e-07,\n",
              "  1.051928848028183e-06,\n",
              "  1.017935574054718e-06,\n",
              "  1.0421499609947205e-06,\n",
              "  1.1194497346878052e-06,\n",
              "  1.11432746052742e-06,\n",
              "  1.0272487998008728e-06,\n",
              "  1.1106021702289581e-06,\n",
              "  9.951181709766388e-07,\n",
              "  9.792856872081757e-07,\n",
              "  9.657815098762512e-07,\n",
              "  1.0165385901927948e-06,\n",
              "  1.0551884770393372e-06,\n",
              "  1.1301599442958832e-06,\n",
              "  9.83942300081253e-07,\n",
              "  1.0677613317966461e-06,\n",
              "  1.050066202878952e-06,\n",
              "  1.0207295417785645e-06,\n",
              "  1.0505318641662598e-06,\n",
              "  1.0309740900993347e-06,\n",
              "  9.997747838497162e-07,\n",
              "  1.0528601706027985e-06,\n",
              "  9.951181709766388e-07,\n",
              "  1.0239891707897186e-06,\n",
              "  1.0617077350616455e-06,\n",
              "  1.0211952030658722e-06,\n",
              "  1.010950654745102e-06,\n",
              "  1.0789372026920319e-06,\n",
              "  1.0691583156585693e-06,\n",
              "  1.0156072676181793e-06,\n",
              "  1.0076910257339478e-06,\n",
              "  1.030508428812027e-06,\n",
              "  1.0686926543712616e-06,\n",
              "  1.0407529771327972e-06,\n",
              "  1.0165385901927948e-06,\n",
              "  1.0710209608078003e-06,\n",
              "  9.257346391677856e-07,\n",
              "  9.64384526014328e-07,\n",
              "  1.0691583156585693e-06,\n",
              "  1.053791493177414e-06,\n",
              "  9.55536961555481e-07,\n",
              "  1.1324882507324219e-06,\n",
              "  1.0640360414981842e-06,\n",
              "  1.003500074148178e-06,\n",
              "  1.0631047189235687e-06,\n",
              "  1.0658986866474152e-06,\n",
              "  9.853392839431763e-07,\n",
              "  1.0044313967227936e-06,\n",
              "  9.82079654932022e-07,\n",
              "  1.0253861546516418e-06,\n",
              "  1.0151416063308716e-06,\n",
              "  9.783543646335602e-07,\n",
              "  1.0509975254535675e-06,\n",
              "  1.0021030902862549e-06,\n",
              "  1.0668300092220306e-06,\n",
              "  1.0570511221885681e-06,\n",
              "  1.0384246706962585e-06,\n",
              "  1.103617250919342e-06,\n",
              "  9.885989129543304e-07,\n",
              "  1.0277144610881805e-06,\n",
              "  1.0617077350616455e-06,\n",
              "  1.0570511221885681e-06,\n",
              "  1.1399388313293457e-06,\n",
              "  1.094769686460495e-06,\n",
              "  9.760260581970215e-07,\n",
              "  9.690411388874054e-07,\n",
              "  9.746290743350983e-07,\n",
              "  1.1278316378593445e-06,\n",
              "  1.037493348121643e-06,\n",
              "  1.0551884770393372e-06,\n",
              "  9.979121387004852e-07,\n",
              "  1.0528601706027985e-06,\n",
              "  1.0342337191104889e-06,\n",
              "  1.092907041311264e-06,\n",
              "  9.960494935512543e-07,\n",
              "  1.0356307029724121e-06,\n",
              "  9.951181709766388e-07,\n",
              "  1.014210283756256e-06,\n",
              "  1.0239891707897186e-06,\n",
              "  1.043081283569336e-06,\n",
              "  9.927898645401e-07,\n",
              "  9.685754776000977e-07,\n",
              "  1.0025687515735626e-06,\n",
              "  9.685754776000977e-07,\n",
              "  1.1119991540908813e-06,\n",
              "  1.0174699127674103e-06,\n",
              "  1.0677613317966461e-06,\n",
              "  1.0025687515735626e-06,\n",
              "  1.0370276868343353e-06,\n",
              "  1.0435469448566437e-06,\n",
              "  1.0351650416851044e-06,\n",
              "  1.0817311704158783e-06,\n",
              "  1.0058283805847168e-06,\n",
              "  1.032371073961258e-06,\n",
              "  1.0598450899124146e-06,\n",
              "  1.016072928905487e-06,\n",
              "  1.0435469448566437e-06,\n",
              "  1.0668300092220306e-06,\n",
              "  1.0207295417785645e-06,\n",
              "  1.1324882507324219e-06,\n",
              "  1.0193325579166412e-06,\n",
              "  1.0509975254535675e-06,\n",
              "  9.83942300081253e-07,\n",
              "  1.0039657354354858e-06,\n",
              "  9.96515154838562e-07,\n",
              "  1.1175870895385742e-06,\n",
              "  1.0388903319835663e-06,\n",
              "  1.017935574054718e-06,\n",
              "  1.0230578482151031e-06,\n",
              "  1.0491348803043365e-06,\n",
              "  1.0533258318901062e-06,\n",
              "  1.0738149285316467e-06,\n",
              "  1.043081283569336e-06,\n",
              "  1.0807998478412628e-06,\n",
              "  1.0016374289989471e-06,\n",
              "  9.848736226558685e-07,\n",
              "  1.0151416063308716e-06,\n",
              "  1.0253861546516418e-06,\n",
              "  1.0384246706962585e-06,\n",
              "  1.0300427675247192e-06,\n",
              "  1.0691583156585693e-06,\n",
              "  1.0868534445762634e-06,\n",
              "  9.19681042432785e-07,\n",
              "  1.0477378964424133e-06,\n",
              "  1.0272487998008728e-06,\n",
              "  1.0617077350616455e-06,\n",
              "  9.597279131412506e-07,\n",
              "  1.069623976945877e-06,\n",
              "  9.685754776000977e-07,\n",
              "  9.89530235528946e-07,\n",
              "  1.0174699127674103e-06,\n",
              "  9.96515154838562e-07,\n",
              "  1.0170042514801025e-06,\n",
              "  1.0253861546516418e-06,\n",
              "  1.016072928905487e-06,\n",
              "  1.0151416063308716e-06,\n",
              "  1.0649673640727997e-06,\n",
              "  1.050066202878952e-06,\n",
              "  1.0170042514801025e-06,\n",
              "  1.0263174772262573e-06,\n",
              "  9.955838322639465e-07,\n",
              "  1.0384246706962585e-06,\n",
              "  1.0891817510128021e-06,\n",
              "  1.1166557669639587e-06,\n",
              "  1.02166086435318e-06,\n",
              "  1.0114163160324097e-06,\n",
              "  1.0263174772262573e-06,\n",
              "  1.0728836059570312e-06,\n",
              "  9.578652679920197e-07,\n",
              "  9.937211871147156e-07,\n",
              "  9.913928806781769e-07,\n",
              "  1.0095536708831787e-06,\n",
              "  5.047768354415894e-07,\n",
              "  4.7497451305389404e-07,\n",
              "  4.870817065238953e-07,\n",
              "  5.098991096019745e-07,\n",
              "  4.838220775127411e-07,\n",
              "  4.670582711696625e-07,\n",
              "  4.912726581096649e-07,\n",
              "  4.773028194904327e-07,\n",
              "  4.684552550315857e-07,\n",
              "  4.6892091631889343e-07,\n",
              "  4.628673195838928e-07,\n",
              "  4.852190613746643e-07,\n",
              "  4.917383193969727e-07,\n",
              "  4.353933036327362e-07,\n",
              "  4.7497451305389404e-07,\n",
              "  5.1083043217659e-07,\n",
              "  4.852190613746643e-07,\n",
              "  4.940666258335114e-07,\n",
              "  4.973262548446655e-07,\n",
              "  4.922039806842804e-07,\n",
              "  4.828907549381256e-07,\n",
              "  4.847534000873566e-07,\n",
              "  4.745088517665863e-07,\n",
              "  5.359761416912079e-07,\n",
              "  4.4517219066619873e-07,\n",
              "  4.773028194904327e-07,\n",
              "  4.9639493227005e-07,\n",
              "  4.78699803352356e-07,\n",
              "  5.094334483146667e-07,\n",
              "  5.010515451431274e-07,\n",
              "  5.182810127735138e-07,\n",
              "  4.637986421585083e-07,\n",
              "  5.145557224750519e-07,\n",
              "  4.7171488404273987e-07,\n",
              "  4.833564162254333e-07,\n",
              "  4.5821070671081543e-07,\n",
              "  4.414469003677368e-07,\n",
              "  4.721805453300476e-07,\n",
              "  4.6798959374427795e-07,\n",
              "  5.024485290050507e-07,\n",
              "  4.591420292854309e-07,\n",
              "  4.987232387065887e-07,\n",
              "  4.3259933590888977e-07,\n",
              "  4.7404319047927856e-07,\n",
              "  4.3725594878196716e-07,\n",
              "  4.87547367811203e-07,\n",
              "  4.670582711696625e-07,\n",
              "  4.721805453300476e-07,\n",
              "  4.614703357219696e-07,\n",
              "  4.66126948595047e-07,\n",
              "  5.033798515796661e-07,\n",
              "  4.707835614681244e-07,\n",
              "  4.55416738986969e-07,\n",
              "  4.7637149691581726e-07,\n",
              "  4.5122578740119934e-07,\n",
              "  4.5588240027427673e-07,\n",
              "  4.628673195838928e-07,\n",
              "  5.061738193035126e-07,\n",
              "  4.773028194904327e-07,\n",
              "  4.852190613746643e-07,\n",
              "  4.7171488404273987e-07,\n",
              "  4.707835614681244e-07,\n",
              "  5.08967787027359e-07,\n",
              "  4.796311259269714e-07,\n",
              "  4.7497451305389404e-07,\n",
              "  4.959292709827423e-07,\n",
              "  4.540197551250458e-07,\n",
              "  4.7171488404273987e-07,\n",
              "  4.684552550315857e-07,\n",
              "  4.852190613746643e-07,\n",
              "  4.637986421585083e-07,\n",
              "  5.122274160385132e-07,\n",
              "  4.76837158203125e-07,\n",
              "  4.6426430344581604e-07,\n",
              "  4.880130290985107e-07,\n",
              "  5.341134965419769e-07,\n",
              "  4.861503839492798e-07,\n",
              "  4.824250936508179e-07,\n",
              "  5.024485290050507e-07,\n",
              "  4.833564162254333e-07,\n",
              "  4.6798959374427795e-07,\n",
              "  4.5588240027427673e-07,\n",
              "  4.577450454235077e-07,\n",
              "  5.350448191165924e-07,\n",
              "  5.08967787027359e-07,\n",
              "  4.936009645462036e-07,\n",
              "  4.898756742477417e-07,\n",
              "  4.6659260988235474e-07,\n",
              "  4.814937710762024e-07,\n",
              "  5.061738193035126e-07,\n",
              "  4.6100467443466187e-07,\n",
              "  4.521571099758148e-07,\n",
              "  4.7404319047927856e-07,\n",
              "  4.78699803352356e-07,\n",
              "  4.530884325504303e-07,\n",
              "  4.87547367811203e-07,\n",
              "  5.210749804973602e-07,\n",
              "  4.9639493227005e-07,\n",
              "  4.605390131473541e-07,\n",
              "  4.7031790018081665e-07,\n",
              "  4.870817065238953e-07,\n",
              "  4.7637149691581726e-07,\n",
              "  4.7031790018081665e-07,\n",
              "  4.842877388000488e-07,\n",
              "  4.912726581096649e-07,\n",
              "  4.7171488404273987e-07,\n",
              "  4.805624485015869e-07,\n",
              "  4.861503839492798e-07,\n",
              "  4.78699803352356e-07,\n",
              "  4.796311259269714e-07,\n",
              "  5.085021257400513e-07,\n",
              "  5.238689482212067e-07,\n",
              "  5.010515451431274e-07,\n",
              "  4.4796615839004517e-07,\n",
              "  4.754401743412018e-07,\n",
              "  5.098991096019745e-07,\n",
              "  4.870817065238953e-07,\n",
              "  5.029141902923584e-07,\n",
              "  5.038455128669739e-07,\n",
              "  4.838220775127411e-07,\n",
              "  4.968605935573578e-07,\n",
              "  4.698522388935089e-07,\n",
              "  4.651956260204315e-07,\n",
              "  4.670582711696625e-07,\n",
              "  5.075708031654358e-07,\n",
              "  4.773028194904327e-07,\n",
              "  4.870817065238953e-07,\n",
              "  4.89410012960434e-07,\n",
              "  4.6193599700927734e-07,\n",
              "  5.047768354415894e-07,\n",
              "  4.85684722661972e-07,\n",
              "  4.5821070671081543e-07,\n",
              "  4.842877388000488e-07,\n",
              "  4.991888999938965e-07,\n",
              "  4.810281097888947e-07,\n",
              "  5.052424967288971e-07,\n",
              "  4.637986421585083e-07,\n",
              "  4.9639493227005e-07,\n",
              "  5.038455128669739e-07,\n",
              "  4.912726581096649e-07,\n",
              "  4.926696419715881e-07,\n",
              "  5.583278834819794e-07,\n",
              "  4.7124922275543213e-07,\n",
              "  4.954636096954346e-07,\n",
              "  4.819594323635101e-07,\n",
              "  4.782341420650482e-07,\n",
              "  4.7171488404273987e-07,\n",
              "  4.866160452365875e-07,\n",
              "  5.331821739673615e-07,\n",
              "  5.010515451431274e-07,\n",
              "  4.6193599700927734e-07,\n",
              "  5.085021257400513e-07,\n",
              "  5.080364644527435e-07,\n",
              "  4.903413355350494e-07,\n",
              "  4.721805453300476e-07,\n",
              "  4.996545612812042e-07,\n",
              "  4.773028194904327e-07,\n",
              "  4.917383193969727e-07,\n",
              "  5.094334483146667e-07,\n",
              "  4.6659260988235474e-07,\n",
              "  4.5821070671081543e-07,\n",
              "  4.880130290985107e-07,\n",
              "  4.651956260204315e-07,\n",
              "  4.6798959374427795e-07,\n",
              "  4.922039806842804e-07,\n",
              "  5.061738193035126e-07,\n",
              "  5.140900611877441e-07,\n",
              "  4.5262277126312256e-07,\n",
              "  4.721805453300476e-07,\n",
              "  4.819594323635101e-07,\n",
              "  5.080364644527435e-07,\n",
              "  5.00120222568512e-07,\n",
              "  5.019828677177429e-07,\n",
              "  4.870817065238953e-07,\n",
              "  4.973262548446655e-07,\n",
              "  5.043111741542816e-07,\n",
              "  4.78699803352356e-07,\n",
              "  4.959292709827423e-07,\n",
              "  4.721805453300476e-07,\n",
              "  4.814937710762024e-07,\n",
              "  4.861503839492798e-07,\n",
              "  4.6659260988235474e-07,\n",
              "  4.647299647331238e-07,\n",
              "  4.498288035392761e-07,\n",
              "  5.010515451431274e-07,\n",
              "  4.6798959374427795e-07,\n",
              "  5.154870450496674e-07,\n",
              "  4.66126948595047e-07,\n",
              "  4.922039806842804e-07,\n",
              "  5.080364644527435e-07,\n",
              "  4.628673195838928e-07,\n",
              "  4.824250936508179e-07,\n",
              "  4.852190613746643e-07,\n",
              "  4.498288035392761e-07,\n",
              "  4.870817065238953e-07,\n",
              "  4.6333298087120056e-07,\n",
              "  4.754401743412018e-07,\n",
              "  5.015172064304352e-07,\n",
              "  4.954636096954346e-07,\n",
              "  4.7497451305389404e-07,\n",
              "  4.977919161319733e-07,\n",
              "  4.954636096954346e-07,\n",
              "  4.945322871208191e-07,\n",
              "  4.880130290985107e-07,\n",
              "  4.6566128730773926e-07,\n",
              "  4.745088517665863e-07,\n",
              "  4.7124922275543213e-07,\n",
              "  4.707835614681244e-07,\n",
              "  4.7264620661735535e-07,\n",
              "  4.870817065238953e-07,\n",
              "  5.154870450496674e-07,\n",
              "  4.833564162254333e-07,\n",
              "  5.010515451431274e-07,\n",
              "  4.498288035392761e-07,\n",
              "  4.6938657760620117e-07,\n",
              "  4.591420292854309e-07,\n",
              "  4.6938657760620117e-07,\n",
              "  4.959292709827423e-07,\n",
              "  4.936009645462036e-07,\n",
              "  5.061738193035126e-07,\n",
              "  4.6426430344581604e-07,\n",
              "  4.945322871208191e-07,\n",
              "  4.98257577419281e-07,\n",
              "  4.3446198105812073e-07,\n",
              "  4.6798959374427795e-07,\n",
              "  4.85684722661972e-07,\n",
              "  5.369074642658234e-07,\n",
              "  5.019828677177429e-07,\n",
              "  4.6798959374427795e-07,\n",
              "  4.880130290985107e-07,\n",
              "  5.187466740608215e-07,\n",
              "  4.861503839492798e-07,\n",
              "  4.773028194904327e-07,\n",
              "  4.470348358154297e-07,\n",
              "  5.033798515796661e-07,\n",
              "  4.805624485015869e-07,\n",
              "  5.07105141878128e-07,\n",
              "  5.159527063369751e-07,\n",
              "  4.796311259269714e-07,\n",
              "  4.76837158203125e-07,\n",
              "  5.075708031654358e-07,\n",
              "  4.4796615839004517e-07,\n",
              "  5.085021257400513e-07,\n",
              "  4.833564162254333e-07,\n",
              "  4.908069968223572e-07,\n",
              "  4.833564162254333e-07,\n",
              "  4.796311259269714e-07,\n",
              "  4.7264620661735535e-07,\n",
              "  5.029141902923584e-07,\n",
              "  4.861503839492798e-07,\n",
              "  4.908069968223572e-07,\n",
              "  4.791654646396637e-07,\n",
              "  4.87547367811203e-07,\n",
              "  4.842877388000488e-07,\n",
              "  4.745088517665863e-07,\n",
              "  4.89410012960434e-07,\n",
              "  4.6798959374427795e-07,\n",
              "  4.6938657760620117e-07,\n",
              "  4.7637149691581726e-07,\n",
              "  5.029141902923584e-07,\n",
              "  4.637986421585083e-07,\n",
              "  4.903413355350494e-07,\n",
              "  5.192123353481293e-07,\n",
              "  4.600733518600464e-07,\n",
              "  4.782341420650482e-07,\n",
              "  4.7171488404273987e-07,\n",
              "  5.331821739673615e-07,\n",
              "  4.7497451305389404e-07,\n",
              "  4.7404319047927856e-07,\n",
              "  5.173496901988983e-07,\n",
              "  4.721805453300476e-07,\n",
              "  4.959292709827423e-07,\n",
              "  4.698522388935089e-07,\n",
              "  4.861503839492798e-07,\n",
              "  5.098991096019745e-07,\n",
              "  4.926696419715881e-07,\n",
              "  5.057081580162048e-07,\n",
              "  4.828907549381256e-07,\n",
              "  4.721805453300476e-07,\n",
              "  4.782341420650482e-07,\n",
              "  4.936009645462036e-07,\n",
              "  4.880130290985107e-07,\n",
              "  4.637986421585083e-07,\n",
              "  4.5867636799812317e-07,\n",
              "  5.033798515796661e-07,\n",
              "  4.6892091631889343e-07,\n",
              "  4.6798959374427795e-07,\n",
              "  5.005858838558197e-07,\n",
              "  4.7124922275543213e-07,\n",
              "  4.87547367811203e-07,\n",
              "  4.833564162254333e-07,\n",
              "  4.922039806842804e-07,\n",
              "  4.949979484081268e-07,\n",
              "  4.889443516731262e-07,\n",
              "  4.5262277126312256e-07,\n",
              "  5.015172064304352e-07,\n",
              "  4.7124922275543213e-07,\n",
              "  5.32250851392746e-07,\n",
              "  4.707835614681244e-07,\n",
              "  5.117617547512054e-07,\n",
              "  4.651956260204315e-07,\n",
              "  5.681067705154419e-07,\n",
              "  4.6798959374427795e-07,\n",
              "  4.735775291919708e-07,\n",
              "  4.3958425521850586e-07,\n",
              "  4.540197551250458e-07,\n",
              "  4.6798959374427795e-07,\n",
              "  4.805624485015869e-07,\n",
              "  4.707835614681244e-07,\n",
              "  4.675239324569702e-07,\n",
              "  4.866160452365875e-07,\n",
              "  5.019828677177429e-07,\n",
              "  4.973262548446655e-07,\n",
              "  4.675239324569702e-07,\n",
              "  5.047768354415894e-07,\n",
              "  4.85684722661972e-07,\n",
              "  4.987232387065887e-07,\n",
              "  4.6566128730773926e-07,\n",
              "  4.87547367811203e-07,\n",
              "  4.903413355350494e-07,\n",
              "  4.76837158203125e-07,\n",
              "  4.316680133342743e-07,\n",
              "  4.6798959374427795e-07,\n",
              "  4.7171488404273987e-07,\n",
              "  4.614703357219696e-07,\n",
              "  4.493631422519684e-07,\n",
              "  4.6193599700927734e-07,\n",
              "  4.540197551250458e-07,\n",
              "  4.852190613746643e-07,\n",
              "  4.968605935573578e-07,\n",
              "  4.759058356285095e-07,\n",
              "  4.7124922275543213e-07,\n",
              "  4.810281097888947e-07,\n",
              "  4.5588240027427673e-07,\n",
              "  5.010515451431274e-07,\n",
              "  4.624016582965851e-07,\n",
              "  5.015172064304352e-07,\n",
              "  4.637986421585083e-07,\n",
              "  4.977919161319733e-07,\n",
              "  5.811452865600586e-07,\n",
              "  4.698522388935089e-07,\n",
              "  4.85684722661972e-07,\n",
              "  4.777684807777405e-07,\n",
              "  4.908069968223572e-07,\n",
              "  4.791654646396637e-07,\n",
              "  4.76837158203125e-07,\n",
              "  4.773028194904327e-07,\n",
              "  4.89410012960434e-07,\n",
              "  4.810281097888947e-07,\n",
              "  4.991888999938965e-07,\n",
              "  4.76837158203125e-07,\n",
              "  4.861503839492798e-07,\n",
              "  5.173496901988983e-07,\n",
              "  5.206093192100525e-07,\n",
              "  4.973262548446655e-07,\n",
              "  4.842877388000488e-07,\n",
              "  5.126930773258209e-07,\n",
              "  4.530884325504303e-07,\n",
              "  5.154870450496674e-07,\n",
              "  4.777684807777405e-07,\n",
              "  4.568137228488922e-07,\n",
              "  4.791654646396637e-07,\n",
              "  4.773028194904327e-07,\n",
              "  4.7637149691581726e-07,\n",
              "  4.66126948595047e-07,\n",
              "  5.061738193035126e-07,\n",
              "  4.7637149691581726e-07,\n",
              "  4.7031790018081665e-07,\n",
              "  5.103647708892822e-07,\n",
              "  4.507601261138916e-07,\n",
              "  4.7124922275543213e-07,\n",
              "  4.5588240027427673e-07,\n",
              "  4.7031790018081665e-07,\n",
              "  4.936009645462036e-07,\n",
              "  4.9639493227005e-07,\n",
              "  4.721805453300476e-07,\n",
              "  4.991888999938965e-07,\n",
              "  5.047768354415894e-07,\n",
              "  4.507601261138916e-07,\n",
              "  5.140900611877441e-07,\n",
              "  4.85684722661972e-07,\n",
              "  4.917383193969727e-07,\n",
              "  4.600733518600464e-07,\n",
              "  4.800967872142792e-07,\n",
              "  4.7637149691581726e-07,\n",
              "  4.614703357219696e-07,\n",
              "  5.289912223815918e-07,\n",
              "  4.824250936508179e-07,\n",
              "  4.940666258335114e-07,\n",
              "  4.684552550315857e-07,\n",
              "  4.698522388935089e-07,\n",
              "  4.605390131473541e-07,\n",
              "  5.005858838558197e-07,\n",
              "  4.903413355350494e-07,\n",
              "  5.024485290050507e-07,\n",
              "  4.866160452365875e-07,\n",
              "  4.912726581096649e-07,\n",
              "  4.777684807777405e-07,\n",
              "  4.6892091631889343e-07,\n",
              "  5.024485290050507e-07,\n",
              "  5.182810127735138e-07,\n",
              "  4.76837158203125e-07,\n",
              "  5.257315933704376e-07,\n",
              "  4.591420292854309e-07,\n",
              "  4.5122578740119934e-07,\n",
              "  4.461035132408142e-07,\n",
              "  4.66126948595047e-07,\n",
              "  5.112960934638977e-07,\n",
              "  5.159527063369751e-07,\n",
              "  4.670582711696625e-07,\n",
              "  4.954636096954346e-07,\n",
              "  4.968605935573578e-07,\n",
              "  4.6659260988235474e-07,\n",
              "  5.103647708892822e-07,\n",
              "  4.735775291919708e-07,\n",
              "  4.647299647331238e-07,\n",
              "  4.745088517665863e-07,\n",
              "  4.777684807777405e-07,\n",
              "  4.651956260204315e-07,\n",
              "  4.889443516731262e-07,\n",
              "  4.745088517665863e-07,\n",
              "  4.782341420650482e-07,\n",
              "  5.010515451431274e-07,\n",
              "  4.912726581096649e-07,\n",
              "  4.735775291919708e-07,\n",
              "  4.6659260988235474e-07,\n",
              "  4.940666258335114e-07,\n",
              "  4.968605935573578e-07,\n",
              "  4.85684722661972e-07,\n",
              "  4.624016582965851e-07,\n",
              "  5.019828677177429e-07,\n",
              "  4.5122578740119934e-07,\n",
              "  4.4424086809158325e-07,\n",
              "  4.931353032588959e-07,\n",
              "  4.922039806842804e-07,\n",
              "  4.423782229423523e-07,\n",
              "  5.243346095085144e-07,\n",
              "  5.038455128669739e-07,\n",
              "  4.600733518600464e-07,\n",
              "  4.7124922275543213e-07,\n",
              "  4.996545612812042e-07,\n",
              "  4.6193599700927734e-07,\n",
              "  4.568137228488922e-07,\n",
              "  4.5495107769966125e-07,\n",
              "  4.805624485015869e-07,\n",
              "  4.6100467443466187e-07,\n",
              "  4.6426430344581604e-07,\n",
              "  4.89410012960434e-07,\n",
              "  4.66126948595047e-07,\n",
              "  4.936009645462036e-07,\n",
              "  5.029141902923584e-07,\n",
              "  4.828907549381256e-07,\n",
              "  5.261972546577454e-07,\n",
              "  ...])"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train(model, optimizer, criterion, 15, train_dataloader, test_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Убедимся, что CUDA доступен\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "model.eval()\n",
        "model.to(device)\n",
        "threshold = 0.5  # Порог можно подобрать на валидационной выборке\n",
        "\n",
        "# Будем хранить тензоры на GPU для вычисления accuracy\n",
        "all_predictions = torch.tensor([], device=device)\n",
        "all_labels = torch.tensor([], device=device)\n",
        "c = 0\n",
        "with torch.no_grad():\n",
        "    for img1, img2, labels in test_loader:\n",
        "        c += 1\n",
        "        # Переносим данные на GPU\n",
        "        img1, img2, labels = img1.to(device), img2.to(device), labels.to(device)\n",
        "\n",
        "        # Получаем эмбеддинги\n",
        "        emb1 = model(img1)\n",
        "        emb2 = model(img2)\n",
        "\n",
        "        # Нормализуем эмбеддинги\n",
        "        emb1 = F.normalize(emb1, p=2, dim=1)\n",
        "        emb2 = F.normalize(emb2, p=2, dim=1)\n",
        "\n",
        "        # Вычисляем косинусную схожесть\n",
        "        cos_sim = F.cosine_similarity(emb1, emb2)\n",
        "\n",
        "        # Преобразуем схожесть в предсказания\n",
        "        predictions = torch.where(cos_sim > threshold, 1, -1)\n",
        "\n",
        "        # Сохраняем на GPU (избегаем передачи CPU)\n",
        "        all_predictions = torch.cat([all_predictions, predictions])\n",
        "        all_labels = torch.cat([all_labels, labels])\n",
        "        if c == 1000:\n",
        "            break\n",
        "\n",
        "# Вычисляем accuracy на GPU\n",
        "correct = (all_predictions == all_labels).float().sum()\n",
        "accuracy = correct / len(all_labels)\n",
        "\n",
        "# Переносим результат на CPU для вывода\n",
        "print(f\"Accuracy: {accuracy.cpu().item():.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FoAgY5ypyWdr",
        "outputId": "d6b5a5c1-294e-4ce6-e5a3-85e3a11a99fa"
      },
      "id": "FoAgY5ypyWdr",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Accuracy: 0.7569\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Вывод:** модель имеет accuracy 0.56 после 9 эпох обучения всех слоев. Это может говорить о том, что архитектура Resnet18 является слишком простой для этой задачи и свертке тяжело дается выделить патерны для лиц.\n",
        "Однако обучение показало лучшие результаты, чем просто модель обученная на ImageNet (0.5 accuracy). Возможно стоит обучить модель на большее количество эпох, однако это крайне проблематично в силу вычислительных ресурсов в моем распоряжении."
      ],
      "metadata": {
        "id": "uOXLSGcA2k_-"
      },
      "id": "uOXLSGcA2k_-"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VIT трансформер"
      ],
      "metadata": {
        "id": "ZDa8h2hozpS8"
      },
      "id": "ZDa8h2hozpS8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Я возьму пример **визуального трансформера** с HuggingFace.\n",
        "\n",
        "По предпололжению, он должен работать лучше в силу внимания.\n",
        "Так же это довольно массивная модель поэтому я буду файнтюнить только последний слой, к тому же модель файнтюнилась на задаче распознавания лиц но на другом датасете."
      ],
      "metadata": {
        "id": "vXfMWD1z0mNC"
      },
      "id": "vXfMWD1z0mNC"
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "import timm\n",
        "\n",
        "token = \"YOUR_TOKEN\"\n",
        "\n",
        "#авторизация\n",
        "login(token)\n",
        "\n",
        "model = timm.create_model(\"hf_hub:gaunernst/vit_tiny_patch8_112.arcface_ms1mv3\", pretrained=True)\n"
      ],
      "metadata": {
        "id": "gGyrpxmbzstZ"
      },
      "id": "gGyrpxmbzstZ",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w = torch.load('/content/sample_data/model_weights_epoch_9.pth',map_location=torch.device('cpu') )\n",
        "model.load_state_dict(w)"
      ],
      "metadata": {
        "id": "8vmTmSwb410s"
      },
      "id": "8vmTmSwb410s",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Обучение:"
      ],
      "metadata": {
        "id": "4AccACnf6of_"
      },
      "id": "4AccACnf6of_"
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn import DataParallel\n",
        "\n",
        "def train(model, optimizer, criterion, epochs, train_dataloader, test_dataloader):\n",
        "    summary_loss_train = []\n",
        "    summary_loss_test = []\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    if torch.cuda.device_count() > 1:\n",
        "        print(f\"Using {torch.cuda.device_count()} GPUs\") # штука для 2 и более гпу\n",
        "        model = DataParallel(model)\n",
        "\n",
        "    for epoch in tqdm(range(epochs), desc=\"Epochs\", ncols=100):\n",
        "        train_loss = 0\n",
        "        train_norm_variable = 0\n",
        "        model.train()\n",
        "        for image0, image1, label in train_dataloader:\n",
        "            image0 = image0.to(device)\n",
        "            image1 = image1.to(device)\n",
        "            label = label.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            embed0 = model(image0)\n",
        "            embed1 = model(image1)\n",
        "\n",
        "            loss = criterion(embed0, embed1, label)\n",
        "            summary_loss_train.append(loss.item())\n",
        "            train_loss += loss.item() * image0.size(0)\n",
        "            train_norm_variable += image0.size(0)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        print(f'Train loss: {train_loss/train_norm_variable}')\n",
        "\n",
        "        torch.save(model.module.state_dict() if isinstance(model, DataParallel) else model.state_dict(),\n",
        "                   f'/kaggle/working/model_weights_epoch_{epoch + 14}.pth')\n",
        "        print(f\"Weights saved for epoch {epoch + 14}\")\n",
        "\n",
        "        test_loss = 0\n",
        "        test_norm_variable = 0\n",
        "        model.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for image0, image1, label in test_dataloader:\n",
        "                image0 = image0.to(device)\n",
        "                image1 = image1.to(device)\n",
        "                label = label.to(device)\n",
        "\n",
        "                embed0 = model(image0)\n",
        "                embed1 = model(image1)\n",
        "                loss = criterion(embed0, embed1, label)\n",
        "                summary_loss_test.append(loss.item())\n",
        "\n",
        "                test_loss += loss.item() * image0.size(0)\n",
        "                test_norm_variable += image0.size(0)\n",
        "\n",
        "        print(f'Test loss: {test_loss/test_norm_variable}')\n",
        "    return summary_loss_train, summary_loss_test"
      ],
      "metadata": {
        "id": "nhA_uiMR6ndF"
      },
      "id": "nhA_uiMR6ndF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=3e-4)\n",
        "criterion = nn.CosineEmbeddingLoss()"
      ],
      "metadata": {
        "id": "xcy2SK0Z6yEj"
      },
      "id": "xcy2SK0Z6yEj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(model, optimizer, criterion, 15, train_dataloader, test_dataloader)"
      ],
      "metadata": {
        "id": "hkcFIDs56yg4"
      },
      "id": "hkcFIDs56yg4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Вывод:** модель показывает результаты значительно лучше. Я могу предроложить, что это связано с вниманием, так как моделт учит не локальный паттерн, а общую взаимосвязь патчей во всем изображении. 0.75"
      ],
      "metadata": {
        "id": "FLFk1Gs56163"
      },
      "id": "FLFk1Gs56163"
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "model.eval()\n",
        "model.to(device)\n",
        "threshold = 0.5\n",
        "\n",
        "\n",
        "all_predictions = torch.tensor([], device=device)\n",
        "all_labels = torch.tensor([], device=device)\n",
        "c = 0\n",
        "with torch.no_grad():\n",
        "    for img1, img2, labels in test_loader:\n",
        "        c += 1\n",
        "        img1, img2, labels = img1.to(device), img2.to(device), labels.to(device)\n",
        "        emb1 = model(img1)\n",
        "        emb2 = model(img2)\n",
        "\n",
        "        emb1 = F.normalize(emb1, p=2, dim=1)\n",
        "        emb2 = F.normalize(emb2, p=2, dim=1)\n",
        "\n",
        "\n",
        "        cos_sim = F.cosine_similarity(emb1, emb2)\n",
        "\n",
        "        predictions = torch.where(cos_sim > threshold, 1, -1)\n",
        "\n",
        "        all_predictions = torch.cat([all_predictions, predictions])\n",
        "        all_labels = torch.cat([all_labels, labels])\n",
        "        if c == 1000:\n",
        "            break\n",
        "\n",
        "correct = (all_predictions == all_labels).float().sum()\n",
        "accuracy = correct / len(all_labels)\n",
        "print(f\"Accuracy: {accuracy.cpu().item():.4f}\")"
      ],
      "metadata": {
        "id": "l7givvQI6d-0"
      },
      "id": "l7givvQI6d-0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Дальнейший код демонстрирует как модель \"видит\" лица в векторном пространстве и какой между нимим будет угол. Можно пронаблюдать, что действительно, у одних лиц косинусная схожесть высока, а у разных маленькая"
      ],
      "metadata": {
        "id": "d_k0jDNy59E2"
      },
      "id": "d_k0jDNy59E2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "284c006d",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-22T00:20:54.747838Z",
          "iopub.status.busy": "2025-04-22T00:20:54.747301Z",
          "iopub.status.idle": "2025-04-22T00:20:55.013439Z",
          "shell.execute_reply": "2025-04-22T00:20:55.012630Z"
        },
        "papermill": {
          "duration": 0.454515,
          "end_time": "2025-04-22T00:20:55.014707",
          "exception": false,
          "start_time": "2025-04-22T00:20:54.560192",
          "status": "completed"
        },
        "tags": [],
        "id": "284c006d",
        "outputId": "2187b584-a9af-458d-88b7-5d823dc062ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'similarity_score': 0.01819482445716858, 'is_similar': False, 'threshold': 0.5}\n"
          ]
        }
      ],
      "source": [
        "def compare_images(\n",
        "    model,          # Ваша модель, возвращающая эмбеддинги\n",
        "    image_path1,    # Путь к первому изображению\n",
        "    image_path2,    # Путь ко второму изображению\n",
        "    transform=None, # Трансформы для изображений (если None - будут созданы автоматически)\n",
        "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
        "    similarity_threshold=0.5  # Порог для принятия решения\n",
        "):\n",
        "    \"\"\"\n",
        "    Сравнивает два изображения с помощью модели.\n",
        "    Возвращает:\n",
        "    - similarity_score (float): косинусная схожесть между эмбеддингами (от -1 до 1)\n",
        "    - is_similar (bool): True если схожесть выше порога\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Подготовка модели\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # 2. Создание трансформов (если не переданы)\n",
        "    if transform is None:\n",
        "        transform = transforms.Compose([\n",
        "            transforms.Resize((112, 112)),  # Размер, ожидаемый моделью\n",
        "            transforms.ToTensor(),\n",
        "            #transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "    # 3. Загрузка и преобразование изображений\n",
        "    def load_image(path):\n",
        "        img = Image.open(path).convert('RGB')\n",
        "        return transform(img).unsqueeze(0).to(device)  # Добавляем batch-размерность\n",
        "\n",
        "    img1 = load_image(image_path1)\n",
        "    img2 = load_image(image_path2)\n",
        "\n",
        "    # 4. Получение эмбеддингов\n",
        "    with torch.no_grad():\n",
        "        emb1 = model(img1)\n",
        "        emb2 = model(img2)\n",
        "\n",
        "    # 5. Нормализация и расчет схожести\n",
        "    emb1 = F.normalize(emb1, p=2, dim=1)\n",
        "    emb2 = F.normalize(emb2, p=2, dim=1)\n",
        "\n",
        "    similarity = F.cosine_similarity(emb1, emb2).item()  # Извлекаем скалярное значение\n",
        "\n",
        "    # 6. Принятие решения\n",
        "    is_similar = similarity > similarity_threshold\n",
        "\n",
        "    return {\n",
        "        'similarity_score': similarity,\n",
        "        'is_similar': is_similar,\n",
        "        'threshold': similarity_threshold\n",
        "    }\n",
        "image_path1 = '/kaggle/input/kryptonit-train/train/images/000003/5.jpg'\n",
        "image_path2 = '/kaggle/input/kryptonit-train/train/images/000003/1.jpg'\n",
        "ans = compare_images(\n",
        "    model,          # Ваша модель, возвращающая эмбеддинги\n",
        "    image_path1,    # Путь к первому изображению\n",
        "    image_path2,    # Путь ко второму изображению\n",
        "    transform=None, # Трансформы для изображений (если None - будут созданы автоматически)\n",
        "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
        "    similarity_threshold=0.5  # Порог для принятия решения\n",
        ")\n",
        "print(ans)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Общий вывод по задаче"
      ],
      "metadata": {
        "id": "HGP4ga4E7pYm"
      },
      "id": "HGP4ga4E7pYm"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Исходя из результатов полученных из валидации моделей я могу сделать вывод что attention лучше работает в этой задаче чем свертки. Так же стоит учесть что эта задача требует gpu и большего количества эпох для обучения, в качестве развития этой задачи можно попробовать обучить эти модели на более мощных ресурсах"
      ],
      "metadata": {
        "id": "KQqegiv-7vGf"
      },
      "id": "KQqegiv-7vGf"
    }
  ],
  "metadata": {
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "datasetId": 26922,
          "sourceId": 34595,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 1365967,
          "sourceId": 2415961,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 4350541,
          "sourceId": 7472837,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 7214126,
          "sourceId": 11505908,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 7214378,
          "sourceId": 11506235,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 7214437,
          "sourceId": 11506316,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 7214493,
          "sourceId": 11506389,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 7214509,
          "sourceId": 11506411,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 6779833,
          "sourceId": 10907467,
          "sourceType": "datasetVersion"
        },
        {
          "sourceId": 16024268,
          "sourceType": "kernelVersion"
        }
      ],
      "dockerImageVersionId": 30918,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 7307.869087,
      "end_time": "2025-04-22T00:20:58.969259",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2025-04-21T22:19:11.100172",
      "version": "2.6.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}